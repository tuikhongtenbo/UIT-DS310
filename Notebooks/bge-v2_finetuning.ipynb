{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85377171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T16:21:38.113072Z",
     "iopub.status.busy": "2025-11-24T16:21:38.112781Z",
     "iopub.status.idle": "2025-11-24T16:21:38.900686Z",
     "shell.execute_reply": "2025-11-24T16:21:38.899949Z",
     "shell.execute_reply.started": "2025-11-24T16:21:38.113050Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë samples: 2189\n",
      "\n",
      "V√≠ d·ª• sample ƒë·∫ßu ti√™n:\n",
      "  Query: Th∆∞a lu·∫≠t s∆∞ t√¥i c√≥ ƒëƒÉng k√Ω k·∫øt h√¥n tr√™n ph√°p lu·∫≠t nh∆∞ng nay v·ª£ ch·ªìng b·ªè nhau theo phong t·ª•c t·∫≠p qu√°...\n",
      "  S·ªë positives: 3\n",
      "  S·ªë negatives: 5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, XLMRobertaForSequenceClassification\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "with open(r'/kaggle/input/law-reranker/train_reranker_main.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"T·ªïng s·ªë samples: {len(data)}\")\n",
    "print(f\"\\nV√≠ d·ª• sample ƒë·∫ßu ti√™n:\")\n",
    "print(f\"  Query: {data[0]['query'][:100]}...\")\n",
    "print(f\"  S·ªë positives: {len(data[0]['pos'])}\")\n",
    "print(f\"  S·ªë negatives: {len(data[0]['neg'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70529abc-9644-4720-83d2-303c85770b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T16:21:04.139027Z",
     "iopub.status.busy": "2025-11-24T16:21:04.138829Z",
     "iopub.status.idle": "2025-11-24T16:21:08.501823Z",
     "shell.execute_reply": "2025-11-24T16:21:08.500888Z",
     "shell.execute_reply.started": "2025-11-24T16:21:04.139010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. H√†m c·∫Øt nh·ªè vƒÉn b·∫£n \n",
    "def clean_and_chunk_legal_text(text):\n",
    "    chunks = []\n",
    "    lines = text.split('\\n')\n",
    "    current_chunk = []\n",
    "    pattern = r'^(\\d+\\.|[a-z]\\)|[\\-\\+])' \n",
    "    header = \"\" \n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        if re.match(pattern, line):\n",
    "            if current_chunk:\n",
    "                full_chunk = \" \".join(current_chunk).strip()\n",
    "                if len(full_chunk.split()) > 5: \n",
    "                    chunks.append((header + \" \" + full_chunk) if header else full_chunk)\n",
    "            current_chunk = [line]\n",
    "        else:\n",
    "            if len(current_chunk) == 0 and not header: header = line\n",
    "            current_chunk.append(line)\n",
    "            \n",
    "    if current_chunk:\n",
    "        full_chunk = \" \".join(current_chunk).strip()\n",
    "        if len(full_chunk.split()) > 5:\n",
    "            chunks.append((header + \" \" + full_chunk) if header else full_chunk)\n",
    "    return chunks if chunks else [text[:2000]]\n",
    "\n",
    "# 2. Load v√† X·ª≠ l√Ω\n",
    "input_path = \"/kaggle/input/law-reranker/train_reranker_main.json\"\n",
    "output_dir = \"/kaggle/working/dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "processed_data = []\n",
    "for item in raw_data:\n",
    "    # Chunking c·∫£ Pos v√† Neg\n",
    "    flat_pos = []\n",
    "    for p in item['pos']: flat_pos.extend(clean_and_chunk_legal_text(p))\n",
    "    \n",
    "    flat_neg = []\n",
    "    for n in item['neg']: flat_neg.extend(clean_and_chunk_legal_text(n))\n",
    "    \n",
    "    if flat_pos and flat_neg:\n",
    "        processed_data.append({\n",
    "            \"query\": item['query'],\n",
    "            \"pos\": flat_pos,      \n",
    "            \"neg\": flat_neg[:15]   \n",
    "        })\n",
    "\n",
    "# 3. Chia Train/Val/Test (80% Train, 10% Val, 10% Test)\n",
    "train_data, temp_data = train_test_split(processed_data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_data)} samples ({len(train_data)/len(processed_data)*100:.1f}%)\")\n",
    "print(f\"Val: {len(val_data)} samples ({len(val_data)/len(processed_data)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_data)} samples ({len(test_data)/len(processed_data)*100:.1f}%)\")\n",
    "\n",
    "# 4. L∆∞u file \n",
    "train_path = f\"{output_dir}/train_reranker_final.json\"\n",
    "val_path = f\"{output_dir}/val_reranker_final.json\"\n",
    "test_path = f\"{output_dir}/test_reranker_final.json\"\n",
    "\n",
    "with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(val_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(test_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff750e-c616-46da-bfa9-22e38648e760",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ce68b-3ed2-40be-9800-15560bbc05e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T16:21:46.974915Z",
     "iopub.status.busy": "2025-11-24T16:21:46.974628Z",
     "iopub.status.idle": "2025-11-24T16:22:27.547743Z",
     "shell.execute_reply": "2025-11-24T16:22:27.546875Z",
     "shell.execute_reply.started": "2025-11-24T16:21:46.974896Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 16:21:56.662705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764001316.898565      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764001316.968279      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bde0f777794d22a4a2b67e50e8f259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e834a47f8f664f72b829b01ecaa2815e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab6f3c5f089445cb7da93d453654598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3a7bb6db974369b2fee5644c27792a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3192e4e0ad4ed3bd9711d100dc8c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/795 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32306dabe6394b40bf77c3a76811b97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(8194, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-reranker-v2-m3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=1, \n",
    "    ignore_mismatched_sizes=True \n",
    ")\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27f88c-b621-434c-a406-7d3d722ba194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T16:22:27.549572Z",
     "iopub.status.busy": "2025-11-24T16:22:27.549005Z",
     "iopub.status.idle": "2025-11-24T16:22:27.865249Z",
     "shell.execute_reply": "2025-11-24T16:22:27.864536Z",
     "shell.execute_reply.started": "2025-11-24T16:22:27.549552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 171]) torch.Size([4, 150])\n"
     ]
    }
   ],
   "source": [
    "class RerankDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.items = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        query = item[\"query\"]\n",
    "        \n",
    "        # L·∫•y t·∫•t c·∫£ positives v√† negatives (kh√¥ng ch·ªâ 1)\n",
    "        pos_list = item[\"pos\"] if isinstance(item[\"pos\"], list) else [item[\"pos\"]]\n",
    "        neg_list = item[\"neg\"] if isinstance(item[\"neg\"], list) else [item[\"neg\"]]\n",
    "        \n",
    "        # Ch·ªçn ng·∫´u nhi√™n 1 pos v√† 1 neg cho m·ªói sample\n",
    "        pos_text = np.random.choice(pos_list)\n",
    "        neg_text = np.random.choice(neg_list)\n",
    "        \n",
    "        return query, pos_text, neg_text\n",
    "\n",
    "def collate_fn(batch):\n",
    "    queries = [b[0] for b in batch]\n",
    "    pos_docs = [b[1] for b in batch]\n",
    "    neg_docs = [b[2] for b in batch]\n",
    "\n",
    "    # Tokenize pairs: [query, doc] cho c·∫£ pos v√† neg\n",
    "    pos_pairs = [[q, d] for q, d in zip(queries, pos_docs)]\n",
    "    neg_pairs = [[q, d] for q, d in zip(queries, neg_docs)]\n",
    "\n",
    "    pos_inputs = tokenizer(pos_pairs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    neg_inputs = tokenizer(neg_pairs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    return pos_inputs, neg_inputs\n",
    "\n",
    "dataset = RerankDataset(\"/kaggle/working/dataset/train_reranker_final.json\") \n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for pos_batch, neg_batch in loader:\n",
    "    print(pos_batch.input_ids.shape, neg_batch.input_ids.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a685b9-56c1-4cd9-b528-be29b145a879",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T16:22:27.866456Z",
     "iopub.status.busy": "2025-11-24T16:22:27.866170Z",
     "iopub.status.idle": "2025-11-24T16:22:27.872890Z",
     "shell.execute_reply": "2025-11-24T16:22:27.872021Z",
     "shell.execute_reply.started": "2025-11-24T16:22:27.866432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.MarginRankingLoss(margin=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ce505",
   "metadata": {},
   "source": [
    "## üìä Ch·ªçn Best Model d·ª±a tr√™n Metric n√†o?\n",
    "\n",
    "**Ch·ªçn Best Model d·ª±a tr√™n: `nDCG@10`** \n",
    "- ƒê√¢y l√† metric t·ªët nh·∫•t cho reranking v√¨ n√≥ xem x√©t c·∫£ v·ªã tr√≠ v√† th·ª© h·∫°ng\n",
    "- N·∫øu nDCG@10 b·∫±ng nhau, c√≥ th·ªÉ xem x√©t MRR@10 ho·∫∑c Accuracy@1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ae487e-1fb3-47fd-adcd-4804889426f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T16:22:27.874690Z",
     "iopub.status.busy": "2025-11-24T16:22:27.874423Z",
     "iopub.status.idle": "2025-11-24T16:22:27.888227Z",
     "shell.execute_reply": "2025-11-24T16:22:27.887458Z",
     "shell.execute_reply.started": "2025-11-24T16:22:27.874674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e850ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m ƒë√°nh gi√° tr√™n validation set\n",
    "def evaluate_on_val(model, tokenizer, val_data_path, device=\"cuda\", top_k=10):\n",
    "    \"\"\"ƒê√°nh gi√° model tr√™n validation set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with open(val_data_path, 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "    \n",
    "    mrr_scores = []\n",
    "    acc_1_scores = []\n",
    "    recall_k_scores = []\n",
    "    ndcg_k_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(val_data, desc=\"Evaluating on Val\"):\n",
    "            query = item['query']\n",
    "            pos_doc = item['pos'][0] if isinstance(item['pos'], list) else item['pos']\n",
    "            neg_docs = item['neg'] if isinstance(item['neg'], list) else [item['neg']]\n",
    "            neg_docs = neg_docs[:top_k-1]\n",
    "            \n",
    "            candidates = [pos_doc] + neg_docs\n",
    "            true_label_index = 0\n",
    "            pairs = [[query, doc] for doc in candidates]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                pairs, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            scores = outputs.logits.view(-1).cpu().numpy()\n",
    "            \n",
    "            sorted_indices = np.argsort(scores)[::-1]\n",
    "            rank = np.where(sorted_indices == true_label_index)[0][0] + 1\n",
    "            \n",
    "            # Acc@1\n",
    "            acc_1_scores.append(1 if rank == 1 else 0)\n",
    "            \n",
    "            # MRR\n",
    "            mrr_scores.append(1 / rank if rank <= top_k else 0)\n",
    "            \n",
    "            # Recall@k\n",
    "            recall_k_scores.append(1 if rank <= top_k else 0)\n",
    "            \n",
    "            # nDCG@k\n",
    "            if rank <= top_k:\n",
    "                dcg = 1.0 / np.log2(rank + 1)\n",
    "                idcg = 1.0 / np.log2(2)\n",
    "                ndcg_k_scores.append(dcg / idcg)\n",
    "            else:\n",
    "                ndcg_k_scores.append(0.0)\n",
    "    \n",
    "    model.train()\n",
    "    return {\n",
    "        \"acc@1\": np.mean(acc_1_scores),\n",
    "        \"mrr@10\": np.mean(mrr_scores),\n",
    "        \"recall@k\": np.mean(recall_k_scores),\n",
    "        \"ndcg@k\": np.mean(ndcg_k_scores)\n",
    "    }\n",
    "\n",
    "# Test h√†m evaluate\n",
    "val_path = \"/kaggle/working/dataset/val_reranker_final.json\"\n",
    "if os.path.exists(val_path):\n",
    "    print(\"‚úÖ Validation set ƒë√£ ƒë∆∞·ª£c t·∫°o\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Validation set ch∆∞a c√≥, c·∫ßn ch·∫°y l·∫°i cell x·ª≠ l√Ω data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e73097-c6e9-4847-9406-e209a27783c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T16:22:27.906398Z",
     "iopub.status.busy": "2025-11-24T16:22:27.906141Z",
     "iopub.status.idle": "2025-11-24T16:56:30.380355Z",
     "shell.execute_reply": "2025-11-24T16:56:30.379745Z",
     "shell.execute_reply.started": "2025-11-24T16:22:27.906375Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493/493 [08:27<00:00,  1.03s/it, loss=0.5493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Avg Loss: 0.5891579867134713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493/493 [08:29<00:00,  1.03s/it, loss=0.2641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Avg Loss: 0.4202923873594337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493/493 [08:31<00:00,  1.04s/it, loss=0.0532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished. Avg Loss: 0.2596782846305459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493/493 [08:34<00:00,  1.04s/it, loss=0.0204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished. Avg Loss: 0.1790790672873999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "val_path = \"/kaggle/working/dataset/val_reranker_final.json\"\n",
    "\n",
    "for epoch in range(4):\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for pos_inputs, neg_inputs in pbar:\n",
    "        # Move to GPU\n",
    "        pos_inputs = {k: v.to(\"cuda\") for k, v in pos_inputs.items()}\n",
    "        neg_inputs = {k: v.to(\"cuda\") for k, v in neg_inputs.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        pos_outputs = model(**pos_inputs)\n",
    "        neg_outputs = model(**neg_inputs)\n",
    "        \n",
    "        # L·∫•y scores (logits)\n",
    "        pos_scores = pos_outputs.logits.squeeze(-1)  # Shape: [batch_size]\n",
    "        neg_scores = neg_outputs.logits.squeeze(-1)  # Shape: [batch_size]\n",
    "        \n",
    "        # Margin Ranking Loss: loss(x1, x2, y) = max(0, -y * (x1 - x2) + margin)\n",
    "        # ·ªû ƒë√¢y: x1 = pos_scores, x2 = neg_scores, y = 1 (v√¨ mu·ªën pos > neg)\n",
    "        # => loss = max(0, -(pos_scores - neg_scores) + margin)\n",
    "        # => loss = max(0, neg_scores - pos_scores + margin)\n",
    "        # M·ª•c ti√™u: pos_scores > neg_scores + margin\n",
    "        target = torch.ones_like(pos_scores)  # y = 1 nghƒ©a l√† pos_scores n√™n l·ªõn h∆°n neg_scores\n",
    "        loss = loss_fn(pos_scores, neg_scores, target)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"\\nEpoch {epoch+1} finished. Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # ƒê√°nh gi√° tr√™n validation set sau m·ªói epoch\n",
    "    if os.path.exists(val_path):\n",
    "        val_metrics = evaluate_on_val(model, tokenizer, val_path, device=\"cuda\")\n",
    "        print(f\"  Val Acc@1: {val_metrics['acc@1']:.4f}, Val MRR@10: {val_metrics['mrr@10']:.4f}\")\n",
    "        print(f\"  Val Recall@10: {val_metrics['recall@k']:.4f}, Val nDCG@10: {val_metrics['ndcg@k']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb6dd6-db9e-47ce-8e35-d5b4c18d1ad0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# L∆∞u model v√† tokenizer\n",
    "output_model_dir = '/kaggle/working/output/model'\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "model.save_pretrained(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)\n",
    "print(f\"‚úÖ Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128b8bc-490e-4309-a439-c2cde7da4cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T16:58:00.613570Z",
     "iopub.status.busy": "2025-11-24T16:58:00.613038Z",
     "iopub.status.idle": "2025-11-24T16:59:10.970532Z",
     "shell.execute_reply": "2025-11-24T16:59:10.969702Z",
     "shell.execute_reply.started": "2025-11-24T16:58:00.613547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë√°nh gi√° tr√™n file: /kaggle/working/dataset/test_reranker_final.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [01:10<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "K·∫æT QU·∫¢ ƒê√ÅNH GI√Å (EVALUATION REPORT)\n",
      "==============================\n",
      "S·ªë l∆∞·ª£ng m·∫´u test: 219\n",
      "Top-10 Candidates mix (1 Pos + 9 Negs)\n",
      "------------------------------\n",
      "‚úÖ Accuracy@1: 0.5388\n",
      "‚úÖ MRR@10   : 0.6727\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# model = XLMRobertaForSequenceClassification.from_pretrained(\"./path_to_saved_model\").to(\"cuda\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./path_to_saved_model\")\n",
    "model.eval()\n",
    "\n",
    "def calculate_metrics(test_data_path, top_k=10):\n",
    "    print(f\"ƒêang ƒë√°nh gi√° tr√™n file: {test_data_path}\")\n",
    "    \n",
    "    with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    mrr_scores = []\n",
    "    acc_1_scores = []\n",
    "    recall_k_scores = []\n",
    "    ndcg_k_scores = []\n",
    "\n",
    "    # Duy·ªát qua t·ª´ng m·∫´u test\n",
    "    for item in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        query = item['query']\n",
    "        \n",
    "        # L·∫•y 1 Positive ƒë·∫°i di·ªán (n·∫øu pos l√† list)\n",
    "        pos_doc = item['pos'][0] if isinstance(item['pos'], list) else item['pos']\n",
    "        # L·∫•y danh s√°ch Negatives\n",
    "        neg_docs = item['neg'] if isinstance(item['neg'], list) else [item['neg']]\n",
    "\n",
    "        neg_docs = neg_docs[:top_k-1] \n",
    "        \n",
    "        # T·∫°o danh s√°ch candidates: [Positive, Neg_1, Neg_2, ..., Neg_9]\n",
    "        # V·ªã tr√≠ th·ª±c s·ª± c·ªßa Positive l√† index 0\n",
    "        candidates = [pos_doc] + neg_docs\n",
    "        true_label_index = 0 \n",
    "        \n",
    "        # T·∫°o c√°c c·∫∑p (Query, Doc) ƒë·ªÉ model ch·∫•m ƒëi·ªÉm\n",
    "        pairs = [[query, doc] for doc in candidates]\n",
    "        \n",
    "        # Tokenize & Predict\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(\n",
    "                pairs, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                return_tensors='pt'\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            scores = outputs.logits.view(-1).cpu().numpy() \n",
    "            \n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        \n",
    "        # T√¨m xem true_label_index (l√† 0) ƒëang n·∫±m ·ªü ƒë√¢u trong danh s√°ch ƒë√£ x·∫øp\n",
    "        rank = np.where(sorted_indices == true_label_index)[0][0] + 1\n",
    "        \n",
    "        # 1. T√≠nh Accuracy@1 \n",
    "        if rank == 1:\n",
    "            acc_1_scores.append(1)\n",
    "        else:\n",
    "            acc_1_scores.append(0)\n",
    "            \n",
    "        # 2. T√≠nh MRR (1/rank)\n",
    "        if rank <= top_k:\n",
    "            mrr_scores.append(1 / rank)\n",
    "        else:\n",
    "            mrr_scores.append(0)\n",
    "        \n",
    "        # 3. T√≠nh Recall@k: N·∫øu positive n·∫±m trong top-k th√¨ = 1, ng∆∞·ª£c l·∫°i = 0\n",
    "        if rank <= top_k:\n",
    "            recall_k_scores.append(1)\n",
    "        else:\n",
    "            recall_k_scores.append(0)\n",
    "        \n",
    "        # 4. T√≠nh nDCG@k\n",
    "        # V·ªõi 1 relevant item: nDCG@k = 1/log2(rank+1) n·∫øu rank <= k, ng∆∞·ª£c l·∫°i = 0\n",
    "        # IDCG@k = 1/log2(2) = 1 (v√¨ relevant item ·ªü v·ªã tr√≠ 1 trong ranking l√Ω t∆∞·ªüng)\n",
    "        if rank <= top_k:\n",
    "            dcg = 1.0 / np.log2(rank + 1)  # relevance = 1 cho positive\n",
    "            idcg = 1.0 / np.log2(2)  # IDCG khi relevant item ·ªü v·ªã tr√≠ 1\n",
    "            ndcg_k_scores.append(dcg / idcg)\n",
    "        else:\n",
    "            ndcg_k_scores.append(0.0)\n",
    "\n",
    "    # T·ªïng k·∫øt\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"K·∫æT QU·∫¢ ƒê√ÅNH GI√Å (EVALUATION REPORT)\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"S·ªë l∆∞·ª£ng m·∫´u test: {len(test_data)}\")\n",
    "    print(f\"Top-{top_k} Candidates mix (1 Pos + {top_k-1} Negs)\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"‚úÖ Accuracy@1: {np.mean(acc_1_scores):.4f}\")\n",
    "    print(f\"‚úÖ MRR@{top_k}   : {np.mean(mrr_scores):.4f}\")\n",
    "    print(f\"‚úÖ Recall@{top_k} : {np.mean(recall_k_scores):.4f}\")\n",
    "    print(f\"‚úÖ nDCG@{top_k}   : {np.mean(ndcg_k_scores):.4f}\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "calculate_metrics(\"/kaggle/working/dataset/test_reranker_final.json\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e9cf1-00d2-48d6-97aa-28a4ede61784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T16:59:16.903612Z",
     "iopub.status.busy": "2025-11-24T16:59:16.902939Z",
     "iopub.status.idle": "2025-11-24T17:00:30.039839Z",
     "shell.execute_reply": "2025-11-24T17:00:30.039039Z",
     "shell.execute_reply.started": "2025-11-24T16:59:16.903588Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ ƒêang t·∫£i Base Model: BAAI/bge-reranker-v2-m3 ...\n",
      "üöÄ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° Base Model tr√™n file: /kaggle/working/dataset/test_reranker_final.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Base Model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [01:09<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üìä K·∫æT QU·∫¢ BASE MODEL (CH∆ØA FINE-TUNE)\n",
      "========================================\n",
      "‚úÖ Accuracy@1: 51.14%\n",
      "‚úÖ MRR@10   : 0.6358\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ==========================================\n",
    "# C·∫§U H√åNH\n",
    "# ==========================================\n",
    "# Load tr·ª±c ti·∫øp t·ª´ HuggingFace thay v√¨ local folder\n",
    "MODEL_NAME = \"BAAI/bge-reranker-v2-m3\" \n",
    "TEST_DATA_PATH = \"/kaggle/working/dataset/test_reranker_final.json\"\n",
    "TOP_K = 10 \n",
    "\n",
    "print(f\"üîÑ ƒêang t·∫£i Base Model: {MODEL_NAME} ...\")\n",
    "\n",
    "# Load Tokenizer & Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "print(f\"üöÄ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° Base Model tr√™n file: {TEST_DATA_PATH}\")\n",
    "\n",
    "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "mrr_scores = []\n",
    "acc_1_scores = []\n",
    "recall_k_scores = []\n",
    "ndcg_k_scores = []\n",
    "\n",
    "# Duy·ªát qua t·ª´ng m·∫´u test\n",
    "for item in tqdm(test_data, desc=\"Evaluating Base Model\"):\n",
    "    query = item['query']\n",
    "    \n",
    "    # L·∫•y 1 Positive + List Negatives\n",
    "    pos_doc = item['pos'][0] if isinstance(item['pos'], list) else item['pos']\n",
    "    neg_docs = item['neg'] if isinstance(item['neg'], list) else [item['neg']]\n",
    "    \n",
    "    # L·∫•y Top K candidates\n",
    "    current_neg_docs = neg_docs[:TOP_K-1]\n",
    "    candidates = [pos_doc] + current_neg_docs\n",
    "    true_label_index = 0 \n",
    "    \n",
    "    # T·∫°o Input Pairs\n",
    "    pairs = [[query, doc] for doc in candidates]\n",
    "    \n",
    "    # D·ª± ƒëo√°n\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            pairs, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors='pt'\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Base model BGE-Reranker xu·∫•t ra logits tr·ª±c ti·∫øp\n",
    "        outputs = model(**inputs)\n",
    "        scores = outputs.logits.view(-1).cpu().numpy()\n",
    "        \n",
    "    # T√≠nh Metrics\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    rank = np.where(sorted_indices == true_label_index)[0][0] + 1\n",
    "    \n",
    "    # Acc@1\n",
    "    if rank == 1:\n",
    "        acc_1_scores.append(1)\n",
    "    else:\n",
    "        acc_1_scores.append(0)\n",
    "        \n",
    "    # MRR\n",
    "    mrr_scores.append(1 / rank)\n",
    "    \n",
    "    # Recall@k: N·∫øu positive n·∫±m trong top-k th√¨ = 1, ng∆∞·ª£c l·∫°i = 0\n",
    "    if rank <= TOP_K:\n",
    "        recall_k_scores.append(1)\n",
    "    else:\n",
    "        recall_k_scores.append(0)\n",
    "    \n",
    "    # nDCG@k\n",
    "    # V·ªõi 1 relevant item: nDCG@k = 1/log2(rank+1) n·∫øu rank <= k, ng∆∞·ª£c l·∫°i = 0\n",
    "    # IDCG@k = 1/log2(2) = 1 (v√¨ relevant item ·ªü v·ªã tr√≠ 1 trong ranking l√Ω t∆∞·ªüng)\n",
    "    if rank <= TOP_K:\n",
    "        dcg = 1.0 / np.log2(rank + 1)  # relevance = 1 cho positive\n",
    "        idcg = 1.0 / np.log2(2)  # IDCG khi relevant item ·ªü v·ªã tr√≠ 1\n",
    "        ndcg_k_scores.append(dcg / idcg)\n",
    "    else:\n",
    "        ndcg_k_scores.append(0.0)\n",
    "\n",
    "# ==========================================\n",
    "# K·∫æT QU·∫¢ SO S√ÅNH\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üìä K·∫æT QU·∫¢ BASE MODEL (CH∆ØA FINE-TUNE)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"‚úÖ Accuracy@1: {np.mean(acc_1_scores) * 100:.2f}%\")\n",
    "print(f\"‚úÖ MRR@{TOP_K}   : {np.mean(mrr_scores):.4f}\")\n",
    "print(f\"‚úÖ Recall@{TOP_K} : {np.mean(recall_k_scores):.4f}\")\n",
    "print(f\"‚úÖ nDCG@{TOP_K}   : {np.mean(ndcg_k_scores):.4f}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47218682",
   "metadata": {},
   "source": [
    "# Finetune BGE Reranker v2 M3 v·ªõi FlagEmbedding\n",
    "\n",
    "Dataset format: `{\"query\": \"...\", \"pos\": [...], \"neg\": [...]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d63a9d-0000-41ef-89bf-8ca6433136f5",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb96add",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# deepspeed_config = {\n",
    "#     \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "#     \"gradient_accumulation_steps\": \"auto\",\n",
    "#     \"zero_optimization\": {\n",
    "#         \"stage\": 2,\n",
    "#         \"allgather_partitions\": True,\n",
    "#         \"allgather_bucket_size\": 200000000,\n",
    "#         \"reduce_scatter\": True,\n",
    "#         \"reduce_bucket_size\": 200000000,\n",
    "#         \"overlap_comm\": True,\n",
    "#         \"contiguous_gradients\": True\n",
    "#     },\n",
    "#     \"fp16\": {\"enabled\": False},\n",
    "#     \"bf16\": {\"enabled\": \"auto\"},\n",
    "#     \"gradient_clipping\": \"auto\",\n",
    "#     \"wall_clock_breakdown\": False\n",
    "# }\n",
    "\n",
    "# import json\n",
    "# with open('stage2.json', 'w') as f:\n",
    "#     json.dump(deepspeed_config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d82cf8",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !python -m torch.distributed.run --nproc_per_node=2 \\\n",
    "#     -m FlagEmbedding.finetune.reranker.encoder_only.base \\\n",
    "#     --output_dir ./output/bge-reranker-v2-m3-finetuned \\\n",
    "#     --model_name_or_path BAAI/bge-reranker-v2-m3 \\\n",
    "#     --train_data /kaggle/working/dataset/train_reranker_final.json \\\n",
    "#     --learning_rate 5e-5 \\\n",
    "#     --num_train_epochs 1 \\\n",
    "#     --per_device_train_batch_size 1 \\\n",
    "#     --gradient_accumulation_steps 8 \\\n",
    "#     --dataloader_drop_last True \\\n",
    "#     --query_max_len 64 \\\n",
    "#     --passage_max_len 512 \\\n",
    "#     --train_group_size 8 \\\n",
    "#     --logging_steps 10 \\\n",
    "#     --save_steps 200 \\\n",
    "#     --save_total_limit 2 \\\n",
    "#     --ddp_find_unused_parameters False \\\n",
    "#     --deepspeed /kaggle/working/stage2.json \\\n",
    "#     --warmup_ratio 0.1 \\\n",
    "#     --bf16 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --overwrite_output_dir \\\n",
    "#     --report_to none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f033b-a10c-4074-b656-1146af740013",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8818537,
     "sourceId": 13845299,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
