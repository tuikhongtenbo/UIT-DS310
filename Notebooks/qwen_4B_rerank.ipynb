{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13960658,"sourceType":"datasetVersion","datasetId":8899185},{"sourceId":14017688,"sourceType":"datasetVersion","datasetId":8929765},{"sourceId":13811466,"sourceType":"datasetVersion","datasetId":8794412}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"25d0b15e","cell_type":"code","source":"# Requires transformers>=4.51.0\nimport torch\nimport json\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\nfrom tqdm import tqdm\n\ndef format_instruction(instruction, query, doc):\n    if instruction is None:\n        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n    return output\n\ndef process_batch_inputs(pairs):\n    \"\"\"Process a batch of query-document pairs\"\"\"\n    inputs = tokenizer(\n        pairs, padding=False, truncation='longest_first',\n        return_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n    )\n    for i, ele in enumerate(inputs['input_ids']):\n        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n    inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n    for key in inputs:\n        inputs[key] = inputs[key].to(model.device)\n    return inputs\n\ndef process_single_input(pair):\n    \"\"\"Process a single query-document pair\"\"\"\n    return process_batch_inputs([pair])\n\n@torch.no_grad()\ndef compute_batch_scores(pairs, batch_size=4):\n    \"\"\"Compute scores for a batch of query-document pairs\n    Uses sum of probabilities for all Yes/No token variants to handle token space issue\n    \"\"\"\n    all_scores = []\n    \n    for i in range(0, len(pairs), batch_size):\n        batch_pairs = pairs[i:i+batch_size]\n        inputs = process_batch_inputs(batch_pairs)\n        logits = model(**inputs).logits[:, -1, :]\n        \n        # Tính tổng xác suất cho tất cả các biến thể Yes và No\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n        \n        batch_scores = []\n        for j in range(len(batch_pairs)):\n            # Cộng dồn xác suất của tất cả token \"Yes\" variants\n            true_prob_sum = 0.0\n            for token_id in token_true_ids:\n                if token_id is not None and token_id < probs.shape[1]:\n                    true_prob_sum += probs[j, token_id].item()\n            \n            # Cộng dồn xác suất của tất cả token \"No\" variants\n            false_prob_sum = 0.0\n            for token_id in token_false_ids:\n                if token_id is not None and token_id < probs.shape[1]:\n                    false_prob_sum += probs[j, token_id].item()\n            \n            # Tính score: P(Yes) / (P(Yes) + P(No))\n            total_prob = true_prob_sum + false_prob_sum\n            if total_prob > 0:\n                score = true_prob_sum / total_prob\n            else:\n                score = 0.5\n            \n            batch_scores.append(score)\n        \n        all_scores.extend(batch_scores)\n        \n        # Free memory\n        del inputs, logits, probs\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    \n    return all_scores\n\n@torch.no_grad()\ndef compute_single_score(pair, debug=False):\n    \"\"\"Compute score for a single query-document pair (wrapper for batch function)\"\"\"\n    scores = compute_batch_scores([pair], batch_size=1)\n    return scores[0]\n\nprint(\"Loading model...\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n    torch.cuda.empty_cache()\n    print(\"Cleared GPU cache\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-4B\", padding_side='left')\n\n# Load model on GPU if available\nif torch.cuda.is_available():\n    model = AutoModelForCausalLM.from_pretrained(\n        \"Qwen/Qwen3-Reranker-4B\",\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    ).eval()\n    print(\"Model loaded on GPU with float16\")\nelse:\n    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-4B\").eval()\n    print(\"Model loaded on CPU\")\n\n# 1. Fix Token Space Issue - Dùng encode để lấy token IDs chính xác hơn\n# Model có thể muốn sinh \"Yes\" hoặc \" Yes\" (có dấu cách trước)\ntoken_yes_list = [\"Yes\", \" Yes\", \"yes\", \" yes\", \"Có\", \" Có\", \"có\", \" có\"]  # Thử cả tiếng Việt\ntoken_no_list = [\"No\", \" No\", \"no\", \" no\", \"Không\", \" Không\", \"không\", \" không\"]\n\n# Lấy tất cả token IDs hợp lệ bằng cách encode (chính xác hơn convert_tokens_to_ids)\ntoken_true_ids = []\ntoken_false_ids = []\n\nprint(f\"\\nToken ID check (using encode method):\")\nfor token in token_yes_list:\n    # Dùng encode và lấy token ID đầu tiên\n    encoded = tokenizer.encode(token, add_special_tokens=False)\n    if encoded:\n        token_id = encoded[0]  # Lấy token ID đầu tiên\n        if token_id not in token_true_ids:\n            token_true_ids.append(token_id)\n            decoded = tokenizer.decode([token_id])\n            print(f\"  Found 'Yes' variant: '{token}' -> ID: {token_id} (decoded: '{decoded}')\")\n\nfor token in token_no_list:\n    encoded = tokenizer.encode(token, add_special_tokens=False)\n    if encoded:\n        token_id = encoded[0]\n        if token_id not in token_false_ids:\n            token_false_ids.append(token_id)\n            decoded = tokenizer.decode([token_id])\n            print(f\"  Found 'No' variant: '{token}' -> ID: {token_id} (decoded: '{decoded}')\")\n\nif not token_true_ids or not token_false_ids:\n    print(\"\\n⚠️ Warning: No valid token IDs found! Trying fallback...\")\n    # Fallback: thử encode trực tiếp\n    try:\n        yes_encoded = tokenizer.encode(\"Yes\", add_special_tokens=False)\n        no_encoded = tokenizer.encode(\"No\", add_special_tokens=False)\n        if yes_encoded:\n            token_true_ids = [yes_encoded[0]]\n        if no_encoded:\n            token_false_ids = [no_encoded[0]]\n    except:\n        pass\n\nprint(f\"\\nUsing {len(token_true_ids)} 'Yes' token(s) and {len(token_false_ids)} 'No' token(s)\")\nif token_true_ids:\n    print(f\"  Yes token IDs: {token_true_ids}\")\nif token_false_ids:\n    print(f\"  No token IDs: {token_false_ids}\")\n\nmax_length = 8192\n\nprefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n# 2. Sửa Suffix - Đơn giản hóa (bỏ <think>)\nsuffix = \"<|im_end|>\\n<|im_start|>assistant\\n\"\nprefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\nsuffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n        \ntask = 'Given a web search query, retrieve relevant passages that answer the query'\n\nprint(f\"\\nPrefix tokens length: {len(prefix_tokens)}\")\nprint(f\"Suffix tokens length: {len(suffix_tokens)}\")\nprint(\"Model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:53:09.272328Z","iopub.execute_input":"2025-12-07T12:53:09.272542Z","iopub.status.idle":"2025-12-07T12:54:17.318371Z","shell.execute_reply.started":"2025-12-07T12:53:09.272497Z","shell.execute_reply":"2025-12-07T12:54:17.317719Z"}},"outputs":[{"name":"stdout","text":"Loading model...\nCUDA available: True\nGPU device: Tesla T4\nGPU memory: 14.74 GB\nCleared GPU cache\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee15a2c57fe24da390ce096f1ce475f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d03f7e781e0542c69a1732bca4bac031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7287fc7e9cd42c18211c9f7cb5f1bd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10470c846f2443d8860b433d79f68c25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7105f6a76ef74054bf6109eec90892d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d38d36349c3487f8ebb40c7226756ff"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-12-07 12:53:28.638448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765112008.825553      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765112008.878319      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8288a2048b24dabb99de956df3131bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ba4912770743a88a46ce8e0624afb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.06G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e02ac0009a42a084315adfed58ae8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feeee8d6bb454e1ab05af098eeb24a45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a7cf77d4fb4bbe9048946a264bb8b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52e91a5687df4841803a98588ba91078"}},"metadata":{}},{"name":"stdout","text":"Model loaded on GPU with float16\n\nToken ID check (using encode method):\n  Found 'Yes' variant: 'Yes' -> ID: 9454 (decoded: 'Yes')\n  Found 'Yes' variant: ' Yes' -> ID: 7414 (decoded: ' Yes')\n  Found 'Yes' variant: 'yes' -> ID: 9693 (decoded: 'yes')\n  Found 'Yes' variant: ' yes' -> ID: 9834 (decoded: ' yes')\n  Found 'Yes' variant: 'Có' -> ID: 34 (decoded: 'C')\n  Found 'Yes' variant: ' Có' -> ID: 129016 (decoded: ' Có')\n  Found 'Yes' variant: 'có' -> ID: 129133 (decoded: 'có')\n  Found 'Yes' variant: ' có' -> ID: 28776 (decoded: ' có')\n  Found 'No' variant: 'No' -> ID: 2753 (decoded: 'No')\n  Found 'No' variant: ' No' -> ID: 2308 (decoded: ' No')\n  Found 'No' variant: 'no' -> ID: 2152 (decoded: 'no')\n  Found 'No' variant: ' no' -> ID: 902 (decoded: ' no')\n  Found 'No' variant: 'Không' -> ID: 142899 (decoded: 'Không')\n  Found 'No' variant: ' Không' -> ID: 129182 (decoded: ' Không')\n  Found 'No' variant: 'không' -> ID: 30664 (decoded: 'kh')\n  Found 'No' variant: ' không' -> ID: 53037 (decoded: ' không')\n\nUsing 8 'Yes' token(s) and 8 'No' token(s)\n  Yes token IDs: [9454, 7414, 9693, 9834, 34, 129016, 129133, 28776]\n  No token IDs: [2753, 2308, 2152, 902, 142899, 129182, 30664, 53037]\n\nPrefix tokens length: 39\nSuffix tokens length: 5\nModel loaded successfully!\n","output_type":"stream"}],"execution_count":1},{"id":"637abb00","cell_type":"code","source":"# Load data files\nprint(\"Loading data files...\")\n\n# Load legal corpus\nLEGAL_CORPUS_PATH = \"/kaggle/input/vlqa-dataset/legal_corpus.json\"\nwith open(LEGAL_CORPUS_PATH, 'r', encoding='utf-8') as f:\n    legal_corpus = json.load(f)\n\n# Build article_id to content mapping\narticle_id_to_content = {}\nfor doc in legal_corpus:\n    for article in doc.get('content', []):\n        aid = article.get('aid')\n        content = article.get('content_Article', '')\n        if aid is not None and content:\n            # Convert aid to string for consistency, but also support int lookup\n            aid_str = str(aid)\n            article_id_to_content[aid_str] = content\n            # Also store with int key if aid is int\n            if isinstance(aid, int):\n                article_id_to_content[aid] = content\n\nprint(f\"Loaded {len(article_id_to_content)} articles from legal corpus\")\n\n# Load private_test.json to get questions\nPRIVATE_TEST_PATH = \"/kaggle/input/test-vlqa/private_test.json\"\nwith open(PRIVATE_TEST_PATH, 'r', encoding='utf-8') as f:\n    private_test_data = json.load(f)\n\n# Build qid to question mapping\nqid_to_question = {}\nfor item in private_test_data:\n    qid = item.get('qid')\n    question = item.get('question', '')\n    if qid is not None and question:\n        qid_to_question[qid] = question\n\nprint(f\"Loaded {len(qid_to_question)} questions from private_test.json\")\n\n# Load results file\nRESULTS_PATH = \"/kaggle/input/reranker-output/results_ensemble_k20.json\"\nwith open(RESULTS_PATH, 'r', encoding='utf-8') as f:\n    results_data = json.load(f)\n\nprint(f\"Loaded {len(results_data)} samples from results file\")\nprint(f\"First sample: qid={results_data[0]['qid']}, num_laws={len(results_data[0]['relevant_laws'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:54:17.319421Z","iopub.execute_input":"2025-12-07T12:54:17.319972Z","iopub.status.idle":"2025-12-07T12:54:19.977521Z","shell.execute_reply.started":"2025-12-07T12:54:17.319953Z","shell.execute_reply":"2025-12-07T12:54:19.976905Z"}},"outputs":[{"name":"stdout","text":"Loading data files...\nLoaded 119270 articles from legal corpus\nLoaded 627 questions from private_test.json\nLoaded 627 samples from results file\nFirst sample: qid=1, num_laws=20\n","output_type":"stream"}],"execution_count":2},{"id":"9e8d72a9","cell_type":"code","source":"# Process all samples and generate top-10 results\nprint(\"=\"*80)\nprint(\"Processing all samples...\")\nprint(\"=\"*80)\n\nfinal_results = []\nfinal_results_with_scores = []  # Results with scores included\ntop_k = 10  # Top-10 results\nbatch_size = 1  # Batch size for faster processing (adjust based on GPU memory)\n\nfor sample_idx, sample in enumerate(tqdm(results_data, desc=\"Processing samples\")):\n    qid = sample['qid']\n    relevant_laws = sample['relevant_laws']  # List of 20 aid values\n    \n    # Load question from private_test.json based on qid\n    query = qid_to_question.get(qid, \"\")\n    if not query:\n        # Try with int/string conversion\n        query = qid_to_question.get(int(qid), \"\") or qid_to_question.get(str(qid), \"\")\n    \n    if not query:\n        # Still add empty result\n        final_results.append({\n            'qid': qid,\n            'relevant_laws': []\n        })\n        final_results_with_scores.append({\n            'qid': qid,\n            'relevant_laws': []\n        })\n        continue\n    \n    # Get documents for the 20 relevant laws\n    documents = []\n    valid_aids = []\n    for aid in relevant_laws:\n        content = article_id_to_content.get(aid) or article_id_to_content.get(str(aid))\n        if content:\n            documents.append(content)\n            valid_aids.append(aid)\n    \n    if not documents:\n        final_results.append({\n            'qid': qid,\n            'relevant_laws': []\n        })\n        final_results_with_scores.append({\n            'qid': qid,\n            'relevant_laws': []\n        })\n        continue\n    \n    # Format pairs for reranking\n    pairs = [format_instruction(task, query, doc) for doc in documents]\n    \n    # Compute scores in batches (faster!)\n    scores = compute_batch_scores(pairs, batch_size=batch_size)\n    \n    # Create results with aid and score\n    results_with_scores = []\n    for aid, score in zip(valid_aids, scores):\n        results_with_scores.append({\n            'aid': aid,\n            'score': score\n        })\n    \n    # Sort by score descending and take top-k\n    results_with_scores.sort(key=lambda x: x['score'], reverse=True)\n    top_k_aids = [item['aid'] for item in results_with_scores[:top_k]]\n    \n    # Add to final results (without scores)\n    final_results.append({\n        'qid': qid,\n        'relevant_laws': top_k_aids\n    })\n    \n    # Add to final results with scores\n    final_results_with_scores.append({\n        'qid': qid,\n        'relevant_laws': [{'aid': item['aid'], 'score': item['score']} for item in results_with_scores[:top_k]]\n    })\n\nprint(f\"\\n{'='*80}\")\nprint(f\"Processing complete! Processed {len(final_results)} samples.\")\nprint(f\"{'='*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:54:19.978256Z","iopub.execute_input":"2025-12-07T12:54:19.978487Z","iopub.status.idle":"2025-12-07T14:14:01.066087Z","shell.execute_reply.started":"2025-12-07T12:54:19.978456Z","shell.execute_reply":"2025-12-07T14:14:01.065149Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nProcessing all samples...\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Processing samples:   0%|          | 0/627 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\nProcessing samples: 100%|██████████| 627/627 [1:19:33<00:00,  7.61s/it]","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nProcessing complete! Processed 627 samples.\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"id":"3430d1d9","cell_type":"code","source":"# Save results to file (without scores - for submission)\nprint(\"\\n\" + \"=\"*80)\nprint(\"Saving results...\")\nprint(\"=\"*80)\n\noutput_file = \"results_qwen_reranker_top10.json\"\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(final_results, f, ensure_ascii=False, indent=2)\n\nprint(f\"Results saved to: {output_file}\")\nprint(f\"Total samples: {len(final_results)}\")\nprint(f\"Format: [{{'qid': int, 'relevant_laws': [int, ...]}}, ...]\")\nprint(f\"Each sample has top-{top_k} relevant laws\")\n\n# Save results with scores (for analysis)\noutput_file_with_scores = \"results_qwen_reranker_top10_with_scores.json\"\nwith open(output_file_with_scores, 'w', encoding='utf-8') as f:\n    json.dump(final_results_with_scores, f, ensure_ascii=False, indent=2)\n\nprint(f\"\\nResults with scores saved to: {output_file_with_scores}\")\nprint(f\"Format: [{{'qid': int, 'relevant_laws': [{{'aid': int, 'score': float}}, ...]}}, ...]\")\n\n# Show statistics\nsamples_with_results = sum(1 for r in final_results if len(r.get('relevant_laws', [])) > 0)\navg_laws = sum(len(r.get('relevant_laws', [])) for r in final_results) / len(final_results) if final_results else 0\n\nprint(f\"\\nStatistics:\")\nprint(f\"  Samples with results: {samples_with_results}/{len(final_results)} ({samples_with_results/len(final_results)*100:.1f}%)\")\nprint(f\"  Average laws per sample: {avg_laws:.2f}\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:14:01.067761Z","iopub.execute_input":"2025-12-07T14:14:01.067975Z","iopub.status.idle":"2025-12-07T14:14:01.118079Z","shell.execute_reply.started":"2025-12-07T14:14:01.067958Z","shell.execute_reply":"2025-12-07T14:14:01.117542Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSaving results...\n================================================================================\nResults saved to: results_qwen_reranker_top10.json\nTotal samples: 627\nFormat: [{'qid': int, 'relevant_laws': [int, ...]}, ...]\nEach sample has top-10 relevant laws\n\nResults with scores saved to: results_qwen_reranker_top10_with_scores.json\nFormat: [{'qid': int, 'relevant_laws': [{'aid': int, 'score': float}, ...]}, ...]\n\nStatistics:\n  Samples with results: 627/627 (100.0%)\n  Average laws per sample: 10.00\n================================================================================\n","output_type":"stream"}],"execution_count":4},{"id":"1542a6e2","cell_type":"code","source":"# Preview first few results\nprint(\"\\n\" + \"=\"*80)\nprint(\"Preview of first 5 results:\")\nprint(\"=\"*80)\nfor i, result in enumerate(final_results[:5]):\n    print(f\"\\n{i+1}. qid={result['qid']}, num_laws={len(result['relevant_laws'])}, top_laws={result['relevant_laws'][:5]}\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:14:01.118633Z","iopub.execute_input":"2025-12-07T14:14:01.118858Z","iopub.status.idle":"2025-12-07T14:14:01.123191Z","shell.execute_reply.started":"2025-12-07T14:14:01.118832Z","shell.execute_reply":"2025-12-07T14:14:01.122562Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nPreview of first 5 results:\n================================================================================\n\n1. qid=1, num_laws=10, top_laws=[2759, 58057, 420, 124, 122]\n\n2. qid=58, num_laws=10, top_laws=[1473, 1480, 504, 1483, 515]\n\n3. qid=60, num_laws=10, top_laws=[643, 1699, 2039, 1453, 1451]\n\n4. qid=114, num_laws=10, top_laws=[12801, 150, 142, 148, 149]\n\n5. qid=124, num_laws=10, top_laws=[36, 2325, 53106, 9189, 2145]\n================================================================================\n","output_type":"stream"}],"execution_count":5},{"id":"45e8be73","cell_type":"code","source":"# Optional: Show sample statistics\nprint(\"\\n\" + \"=\"*80)\nprint(\"Sample Statistics\")\nprint(\"=\"*80)\n\n# Count distribution of number of laws\nlaw_counts = {}\nfor result in final_results:\n    count = len(result.get('relevant_laws', []))\n    law_counts[count] = law_counts.get(count, 0) + 1\n\nprint(\"\\nDistribution of number of laws per sample:\")\nfor count in sorted(law_counts.keys()):\n    print(f\"  {count} laws: {law_counts[count]} samples\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:14:01.123802Z","iopub.execute_input":"2025-12-07T14:14:01.124021Z","iopub.status.idle":"2025-12-07T14:14:01.142953Z","shell.execute_reply.started":"2025-12-07T14:14:01.124006Z","shell.execute_reply":"2025-12-07T14:14:01.142256Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSample Statistics\n================================================================================\n\nDistribution of number of laws per sample:\n  10 laws: 627 samples\n================================================================================\n","output_type":"stream"}],"execution_count":6},{"id":"e767c6c9-22c5-4c22-b846-8f141f2ca68d","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}