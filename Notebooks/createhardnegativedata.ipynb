{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13811466,"sourceType":"datasetVersion","datasetId":8794412}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2b099413","cell_type":"code","source":"%pip install rank-bm25\n%pip install -U sentence-transformers\n%pip install chromadb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-23T14:30:46.696237Z","iopub.execute_input":"2025-11-23T14:30:46.696443Z","iopub.status.idle":"2025-11-23T14:32:28.026154Z","shell.execute_reply.started":"2025-11-23T14:30:46.696417Z","shell.execute_reply":"2025-11-23T14:32:28.025192Z"},"id":"2b099413","outputId":"a65006db-d302-4e05-8197-ef939ef9e4a0","trusted":true},"outputs":[{"name":"stdout","text":"Collecting rank-bm25\n  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank-bm25) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank-bm25) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank-bm25) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank-bm25) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rank-bm25) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rank-bm25) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rank-bm25) (2024.2.0)\nDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\nInstalling collected packages: rank-bm25\nSuccessfully installed rank-bm25-0.2.2\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nCollecting sentence-transformers\n  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nDownloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.0/488.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 4.1.0\n    Uninstalling sentence-transformers-4.1.0:\n      Successfully uninstalled sentence-transformers-4.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-5.1.2\nNote: you may need to restart the kernel to use updated packages.\nCollecting chromadb\n  Downloading chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.12.4)\nCollecting pybase64>=1.4.1 (from chromadb)\n  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\nRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\nCollecting posthog<6.0.0,>=2.4.0 (from chromadb)\n  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.15.0)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.37.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.37.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.74.0)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\nRequirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.3)\nCollecting mmh3>=4.0.1 (from chromadb)\n  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.11.0)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.2.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.25.0)\nRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (25.0)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nCollecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (6.33.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\nRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\nRequirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb)\n  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading httptools-0.7.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\nCollecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvloop-0.22.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading watchfiles-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\nCollecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.10.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\nDownloading chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\nDownloading httptools-0.7.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (456 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.6/456.6 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.22.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.1/456.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=f80705ff947b685020f64368a2093cb6647cb68fe362b50690756d89401f6999\n  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\nSuccessfully built pypika\nInstalling collected packages: pypika, durationpy, uvloop, urllib3, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, cachetools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-semantic-conventions, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, onnxruntime, chromadb\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.5.0\n    Uninstalling urllib3-2.5.0:\n      Successfully uninstalled urllib3-2.5.0\n  Attempting uninstall: opentelemetry-proto\n    Found existing installation: opentelemetry-proto 1.37.0\n    Uninstalling opentelemetry-proto-1.37.0:\n      Successfully uninstalled opentelemetry-proto-1.37.0\n  Attempting uninstall: cachetools\n    Found existing installation: cachetools 6.2.1\n    Uninstalling cachetools-6.2.1:\n      Successfully uninstalled cachetools-6.2.1\n  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.37.0\n    Uninstalling opentelemetry-api-1.37.0:\n      Successfully uninstalled opentelemetry-api-1.37.0\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.37.0\n    Uninstalling opentelemetry-sdk-1.37.0:\n      Successfully uninstalled opentelemetry-sdk-1.37.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-adk 1.18.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\ngoogle-adk 1.18.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 cachetools-5.5.2 chromadb-1.3.5 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"id":"ccbf26df","cell_type":"code","source":"%pip install nltk","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-23T14:32:28.028167Z","iopub.execute_input":"2025-11-23T14:32:28.028413Z","iopub.status.idle":"2025-11-23T14:32:31.359472Z","shell.execute_reply.started":"2025-11-23T14:32:28.028389Z","shell.execute_reply":"2025-11-23T14:32:31.358508Z"},"id":"ccbf26df","outputId":"c6f13cfd-8a80-45c2-b4f5-e4fabb227c9c","trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"id":"f7bda426","cell_type":"code","source":"import json\nimport os\nimport unicodedata\nfrom rank_bm25 import BM25Okapi\nfrom tqdm import tqdm\nfrom typing import List, Dict, Tuple\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport chromadb","metadata":{"execution":{"iopub.status.busy":"2025-11-23T14:32:31.360695Z","iopub.execute_input":"2025-11-23T14:32:31.361000Z","iopub.status.idle":"2025-11-23T14:33:12.909488Z","shell.execute_reply.started":"2025-11-23T14:32:31.360968Z","shell.execute_reply":"2025-11-23T14:33:12.908875Z"},"id":"f7bda426","trusted":true},"outputs":[{"name":"stderr","text":"2025-11-23 14:32:45.147133: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763908365.362669      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763908365.425471      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":3},{"id":"13tWRcq0Pm3y","cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-23T14:33:12.910247Z","iopub.execute_input":"2025-11-23T14:33:12.910475Z","iopub.status.idle":"2025-11-23T14:33:13.020608Z","shell.execute_reply.started":"2025-11-23T14:33:12.910455Z","shell.execute_reply":"2025-11-23T14:33:13.019760Z"},"id":"13tWRcq0Pm3y","outputId":"6fbb2c74-65bb-4dd1-beab-4d9563b39691","trusted":true},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"id":"4016fc05","cell_type":"code","source":"TRAIN_FILE = \"/kaggle/input/vlqa-dataset/train.json\"\nLEGAL_CORPUS_FILE = \"/kaggle/input/vlqa-dataset/legal_corpus.json\"\nOUTPUT_FILE = \"/kaggle/working/Data/hard_negative.json\"","metadata":{"execution":{"iopub.status.busy":"2025-11-23T14:33:13.021497Z","iopub.execute_input":"2025-11-23T14:33:13.021803Z","iopub.status.idle":"2025-11-23T14:33:13.026315Z","shell.execute_reply.started":"2025-11-23T14:33:13.021776Z","shell.execute_reply":"2025-11-23T14:33:13.025527Z"},"id":"4016fc05","trusted":true},"outputs":[],"execution_count":5},{"id":"5a3a7a90","cell_type":"code","source":"# Dense Retrieval Model \nembedding_model = SentenceTransformer(\"AITeamVN/Vietnamese_Embedding\", device=\"cuda\")\nembedding_model.half()\n\n# Chunking settings\nCHUNK_SIZE = 2048  \nCHUNK_OVERLAP = 128\n\n# Retrieval settings\nBM25_TOP_K = 20  # Top 20 negatives từ BM25\nDENSE_RERANK_TOP_K = 5  # Top 5 sau khi rerank bằng dense retrieval ","metadata":{"execution":{"iopub.status.busy":"2025-11-23T14:33:13.027211Z","iopub.execute_input":"2025-11-23T14:33:13.027502Z","iopub.status.idle":"2025-11-23T14:33:29.000287Z","shell.execute_reply.started":"2025-11-23T14:33:13.027469Z","shell.execute_reply":"2025-11-23T14:33:28.999632Z"},"id":"5a3a7a90","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c5e7765dd2a4001960daa570e00a5d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90ed27a3879c4771887b5790d8477079"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f97dc18b57e2497e8a8478ecbe5374d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"985a46fdf55f4f0da640f42dd1713b5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/708 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e34c1b238ba41f58fe8bdc85884c501"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a33db9bb66d8407395949d53a9714f15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"415b547f38ab43f4be3561a36d405f67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3be88ca07674566896c93a002b878dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa2bdcc35a4047539389827ea0f153c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed2d49d26774199b61c53375019e13a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d56ea66634d6464c975e9873116a07e5"}},"metadata":{}}],"execution_count":6},{"id":"7214a94b","cell_type":"code","source":"# Load training data\nwith open(TRAIN_FILE, 'r', encoding='utf-8') as f:\n    train_data = json.load(f)\n\n# Load legal corpus\nwith open(LEGAL_CORPUS_FILE, 'r', encoding='utf-8') as f:\n    legal_corpus = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2025-11-23T14:33:29.002435Z","iopub.execute_input":"2025-11-23T14:33:29.002661Z","iopub.status.idle":"2025-11-23T14:33:31.545467Z","shell.execute_reply.started":"2025-11-23T14:33:29.002638Z","shell.execute_reply":"2025-11-23T14:33:31.544484Z"},"id":"7214a94b","trusted":true},"outputs":[],"execution_count":7},{"id":"1f391b58","cell_type":"code","source":"# mapping: article_id -> article_content\narticle_id_to_content = {}\narticle_id_to_text = {}\nall_article_ids = []\n\nfor doc in tqdm(legal_corpus, desc=\"Procesisng legal corpus\"):\n    for article in doc.get('content', []):\n        aid = article.get('aid')\n        content = article.get('content_Article', '')\n        if aid is not None and content:\n            article_id_to_content[aid] = content\n            article_id_to_text[aid] = content\n            all_article_ids.append(aid)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-23T14:33:31.546464Z","iopub.execute_input":"2025-11-23T14:33:31.546813Z","iopub.status.idle":"2025-11-23T14:33:31.627971Z","shell.execute_reply.started":"2025-11-23T14:33:31.546779Z","shell.execute_reply":"2025-11-23T14:33:31.627133Z"},"id":"1f391b58","outputId":"ab5638a7-33a2-449f-ca82-ecf0c290ac9d","trusted":true},"outputs":[{"name":"stderr","text":"Procesisng legal corpus: 100%|██████████| 2157/2157 [00:00<00:00, 30943.63it/s]\n","output_type":"stream"}],"execution_count":8},{"id":"ab1cdc53","cell_type":"code","source":"def chunk_with_overlap(document: str, chunk_size: int = 2000, overlap: int = 200) -> List[str]:\n    \"\"\"\n    Chunk document into long chunks with overlap.\n    Split at line breaks (\\n) to maintain semantic continuity.\n    \n    Args:\n        document: Input document text\n        chunk_size: Maximum size of each chunk (characters)\n        overlap: Number of overlapping characters between chunks\n    \n    Returns:\n        List of text chunks\n    \"\"\"\n    if not document:\n        return []\n    \n    # Split document by line breaks first\n    lines = document.split('\\n')\n    \n    chunks = []\n    current_chunk = \"\"\n    \n    for line in lines:\n        # If adding this line would exceed chunk_size\n        if len(current_chunk) + len(line) + 1 > chunk_size:\n            if current_chunk:\n                chunks.append(current_chunk.strip())\n            \n            # Start new chunk with overlap from previous chunk\n            if chunks and overlap > 0:\n                # Take last 'overlap' characters from previous chunk\n                prev_chunk = chunks[-1]\n                overlap_text = prev_chunk[-overlap:] if len(prev_chunk) > overlap else prev_chunk\n                current_chunk = overlap_text + \"\\n\" + line\n            else:\n                current_chunk = line\n        else:\n            if current_chunk:\n                current_chunk += \"\\n\" + line\n            else:\n                current_chunk = line\n    \n    # Add the last chunk\n    if current_chunk:\n        chunks.append(current_chunk.strip())\n    \n    return chunks\n    \ndef preprocess(sentence: str) -> str:\n    \"\"\"Preprocess text: normalize, clean, remove special characters\"\"\"\n    if not isinstance(sentence, str):\n        return \"\"\n\n    # Normalize Unicode\n    sentence = unicodedata.normalize(\"NFC\", sentence)\n\n    # Lowercase\n    sentence = sentence.lower().strip()\n\n    # Remove URLs\n    sentence = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", sentence)\n\n    # Remove email\n    sentence = re.sub(r\"\\S+@\\S+\", \"\", sentence)\n\n    # Remove special characters\n    sentence = re.sub(r\"[^a-zA-Z0-9À-ỹ ]+\", \" \", sentence)\n\n    # Remove extra whitespace\n    sentence = \" \".join(sentence.split())\n\n    return sentence\n\ndef tokenize(sentence: str) -> List[str]:\n\n    if not sentence:\n        return []\n\n    # Preprocess\n    sentence = preprocess(sentence)\n\n    if not sentence:\n        return []\n\n    # NLTK word_tokenize\n    result = word_tokenize(sentence)\n\n    return result\n\n# Test preprocessing & tokenization\ntest_text = \"Thưa luật sư tôi có đăng; ký kết hôn (theo quy định tại Điều 1)\"\nprint(f\"Original: {test_text}\")\nprint(f\"Preprocessed: {preprocess(test_text)}\")\nprint(f\"Tokenized: {tokenize(test_text)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-23T14:33:31.629214Z","iopub.execute_input":"2025-11-23T14:33:31.629531Z","iopub.status.idle":"2025-11-23T14:33:38.313236Z","shell.execute_reply.started":"2025-11-23T14:33:31.629504Z","shell.execute_reply":"2025-11-23T14:33:38.312275Z"},"id":"ab1cdc53","outputId":"852981d4-0927-4b9b-f90a-221ad7af2f27","trusted":true},"outputs":[{"name":"stdout","text":"Original: Thưa luật sư tôi có đăng; ký kết hôn (theo quy định tại Điều 1)\nPreprocessed: thưa luật sư tôi có đăng ký kết hôn theo quy định tại điều 1\nTokenized: ['thưa', 'luật', 'sư', 'tôi', 'có', 'đăng', 'ký', 'kết', 'hôn', 'theo', 'quy', 'định', 'tại', 'điều', '1']\n","output_type":"stream"}],"execution_count":9},{"id":"4f435454","cell_type":"code","source":"# Chunking articles thành các đoạn dài (2048 chars với overlap)\nchunked_data = []  # List of (chunk_id, article_id, chunk_text)\nchunk_id_to_article_id = {}  # Mapping từ chunk_id -> article_id\n\nchunk_id = 0\nfor aid in tqdm(all_article_ids, desc=\"Chunking articles\"):\n    content = article_id_to_text[aid]\n    chunks = chunk_with_overlap(content, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\n    \n    for chunk_text in chunks:\n        if chunk_text.strip():  \n            chunked_data.append({\n                'chunk_id': chunk_id,\n                'article_id': aid,\n                'chunk_text': chunk_text\n            })\n            chunk_id_to_article_id[chunk_id] = aid\n            chunk_id += 1\n\ntokenized_documents = []\narticle_ids_list = []\n\nfor aid in tqdm(all_article_ids, desc=\"Tokenizing articles\"):\n    content = article_id_to_text[aid]\n    tokens = tokenize(content)\n    tokenized_documents.append(tokens)\n    article_ids_list.append(aid)\n\n# Build BM25 index (Statistical Retrieval Model)\nbm25 = BM25Okapi(tokenized_documents)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-23T14:33:38.317539Z","iopub.execute_input":"2025-11-23T14:33:38.317829Z","iopub.status.idle":"2025-11-23T14:34:43.489273Z","shell.execute_reply.started":"2025-11-23T14:33:38.317801Z","shell.execute_reply":"2025-11-23T14:34:43.488394Z"},"id":"4f435454","outputId":"67b12ac5-004b-4f6f-a0fd-dfbaa54c6868","trusted":true},"outputs":[{"name":"stderr","text":"Chunking articles: 100%|██████████| 59635/59635 [00:00<00:00, 114006.09it/s]\nTokenizing articles: 100%|██████████| 59635/59635 [00:59<00:00, 997.82it/s] \n","output_type":"stream"}],"execution_count":10},{"id":"vRIFNXsUR_Km","cell_type":"code","source":"# Embedding chunks và lưu vào ChromaDB\n# Init ChromaDB \nchroma_client = chromadb.PersistentClient(path=\"/kaggle/working/chroma_db\")\ncollection_name = \"legal_articles_chunks\"\n\n# Load hoặc tạo collection\ntry:\n    collection = chroma_client.get_collection(name=collection_name)\n    collection_count = collection.count()\n    need_to_add = collection_count < len(chunked_data)\n    print(f\"Collection đã tồn tại với {collection_count} chunks\")\nexcept:\n    collection = chroma_client.create_collection(\n        name=collection_name,\n        metadata={\"hnsw:space\": \"cosine\"}\n    )\n    need_to_add = True\n    print(\"Tạo collection mới\")\n\n# Nếu cần thì embed và add theo batch nhỏ để tránh lỗi\nif need_to_add:\n    chunk_texts = [item['chunk_text'] for item in chunked_data]\n    chunk_ids   = [str(item['chunk_id']) for item in chunked_data]\n    metadatas   = [\n        {\n            'article_id': str(item['article_id']),\n            'chunk_text': item['chunk_text'][:500]\n        }\n        for item in chunked_data\n    ]\n\n    # Tạo embeddings theo batch\n    embedding_batch_size = 8\n    db_batch_size = 5000  # Batch size nhỏ hơn để tránh lỗi ChromaDB\n    \n    all_embeddings = []\n    for i in tqdm(range(0, len(chunk_texts), embedding_batch_size), desc=\"Embedding chunks\"):\n        batch = chunk_texts[i:i+embedding_batch_size]\n        vecs = embedding_model.encode(batch, show_progress_bar=False)\n        all_embeddings.extend(vecs.tolist())\n\n    # Add vào ChromaDB theo batch nhỏ\n    for i in tqdm(range(0, len(chunk_ids), db_batch_size), desc=\"Adding to ChromaDB\"):\n        batch_ids = chunk_ids[i:i+db_batch_size]\n        batch_embeddings = all_embeddings[i:i+db_batch_size]\n        batch_documents = chunk_texts[i:i+db_batch_size]\n        batch_metadatas = metadatas[i:i+db_batch_size]\n        \n        collection.add(\n            ids=batch_ids,\n            embeddings=batch_embeddings,\n            documents=batch_documents,\n            metadatas=batch_metadatas\n        )\n    \n    print(f\"✅ Đã thêm {len(chunk_ids)} chunks vào ChromaDB\")\n\n# Load mapping chunk_id -> article_id từ ChromaDB\nexisting = collection.get()\nfor cid, meta in zip(existing['ids'], existing['metadatas']):\n    if meta.get('article_id') is not None:\n        chunk_id_to_article_id[int(cid)] = int(meta['article_id'])\n\nprint(f\"✅ Đã tải {len(chunk_id_to_article_id)} mappings từ ChromaDB\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"execution":{"iopub.status.busy":"2025-11-23T14:34:49.390603Z","iopub.execute_input":"2025-11-23T14:34:49.390894Z","iopub.status.idle":"2025-11-23T15:09:11.027217Z","shell.execute_reply.started":"2025-11-23T14:34:49.390873Z","shell.execute_reply":"2025-11-23T15:09:11.026365Z"},"id":"vRIFNXsUR_Km","outputId":"15c5da02-599d-4047-8823-c50739536ee4","trusted":true},"outputs":[{"name":"stdout","text":"Tạo collection mới\n","output_type":"stream"},{"name":"stderr","text":"Embedding chunks: 100%|██████████| 9779/9779 [31:41<00:00,  5.14it/s]  \nAdding to ChromaDB: 100%|██████████| 16/16 [02:35<00:00,  9.74s/it]\n","output_type":"stream"},{"name":"stdout","text":"✅ Đã thêm 78229 chunks vào ChromaDB\n✅ Đã tải 78229 mappings từ ChromaDB\n","output_type":"stream"}],"execution_count":11},{"id":"4600b42b","cell_type":"code","source":"# Hàm retrieve top-k articles từ ChromaDB (không dùng RAM)\ndef retrieve_top_k_from_chromadb(question: str, top_k: int = 100) -> List[Tuple[int, float]]:\n    \"\"\"\n    Retrieve top-k chunks từ ChromaDB sử dụng dense retrieval.\n    Không load embeddings vào RAM, query trực tiếp từ ChromaDB.\n    \n    Returns:\n        List of tuples (article_id, score) sorted by score descending\n    \"\"\"\n    # Encode question\n    query_embedding = embedding_model.encode([question], show_progress_bar=False)[0]\n    \n    # Query ChromaDB\n    results = collection.query(\n        query_embeddings=[query_embedding.tolist()],\n        n_results=top_k\n    )\n    \n    # Extract article_ids và scores\n    retrieved_articles = []\n    if results['ids'] and len(results['ids'][0]) > 0:\n        for i, chunk_id in enumerate(results['ids'][0]):\n            article_id = chunk_id_to_article_id.get(int(chunk_id))\n            if article_id is not None:\n                # Lấy distance từ ChromaDB (cosine distance)\n                distance = results['distances'][0][i] if 'distances' in results else 0.0\n                # Convert distance to similarity score (1 - distance for cosine)\n                score = 1.0 - distance\n                retrieved_articles.append((article_id, score))\n    \n    # Remove duplicates và sort by score\n    article_scores = {}\n    for aid, score in retrieved_articles:\n        if aid not in article_scores or score > article_scores[aid]:\n            article_scores[aid] = score\n    \n    results_sorted = sorted(article_scores.items(), key=lambda x: x[1], reverse=True)\n    return results_sorted[:top_k]\n\n# Hàm retrieve top-k articles với BM25\ndef retrieve_top_k_bm25(question: str, top_k: int = 100) -> List[Tuple[int, float]]:\n    \"\"\"\n    Retrieve top-k articles sử dụng BM25.\n    \n    Returns:\n        List of tuples (article_id, bm25_score) sorted by score descending\n    \"\"\"\n    query_tokens = tokenize(question)\n    bm25_scores = bm25.get_scores(query_tokens)\n    \n    # Tạo list (article_id, bm25_score) và sort\n    results = [(article_ids_list[i], float(bm25_scores[i])) for i in range(len(bm25_scores))]\n    results.sort(key=lambda x: x[1], reverse=True)\n    \n    return results[:top_k]\n\n# Test retrieval\ntest_question = train_data[0]['question']\nprint(f\"\\nTest question: {test_question[:100]}...\")\n\n# Test BM25\nbm25_results = retrieve_top_k_bm25(test_question, top_k=10)\nprint(f\"\\nTop 10 retrieved articles (BM25):\")\nfor aid, score in bm25_results[:10]:\n    print(f\"  Article ID: {aid}, BM25 Score: {score:.4f}\")\n\n# Test ChromaDB\nchromadb_results = retrieve_top_k_from_chromadb(test_question, top_k=10)\nprint(f\"\\nTop 10 retrieved articles (ChromaDB Dense Retrieval):\")\nfor aid, score in chromadb_results[:10]:\n    print(f\"  Article ID: {aid}, Similarity Score: {score:.4f}\")","metadata":{"id":"4600b42b","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T15:09:32.156437Z","iopub.execute_input":"2025-11-23T15:09:32.157000Z","iopub.status.idle":"2025-11-23T15:09:33.149475Z","shell.execute_reply.started":"2025-11-23T15:09:32.156977Z","shell.execute_reply":"2025-11-23T15:09:33.148809Z"}},"outputs":[{"name":"stdout","text":"\nTest question: Thưa luật sư tôi có đăng ký kết hôn trên pháp luật nhưng nay vợ chồng bỏ nhau theo phong tục tập quá...\n\nTop 10 retrieved articles (BM25):\n  Article ID: 3441, BM25 Score: 85.9256\n  Article ID: 53886, BM25 Score: 85.0300\n  Article ID: 6740, BM25 Score: 83.5712\n  Article ID: 3424, BM25 Score: 83.2882\n  Article ID: 5850, BM25 Score: 82.1975\n  Article ID: 53875, BM25 Score: 81.8095\n  Article ID: 5856, BM25 Score: 80.5794\n  Article ID: 5849, BM25 Score: 79.6637\n  Article ID: 53939, BM25 Score: 79.5563\n  Article ID: 53877, BM25 Score: 75.5946\n\nTop 10 retrieved articles (ChromaDB Dense Retrieval):\n  Article ID: 57800, Similarity Score: 0.5199\n  Article ID: 53875, Similarity Score: 0.4965\n  Article ID: 53877, Similarity Score: 0.4689\n  Article ID: 53881, Similarity Score: 0.4683\n  Article ID: 53886, Similarity Score: 0.4535\n  Article ID: 53880, Similarity Score: 0.4506\n  Article ID: 53882, Similarity Score: 0.4468\n  Article ID: 5168, Similarity Score: 0.4380\n  Article ID: 53894, Similarity Score: 0.4222\n  Article ID: 52776, Similarity Score: 0.4172\n","output_type":"stream"}],"execution_count":12},{"id":"16813f5d","cell_type":"code","source":"# BATCH PROCESSING VERSION - Tối ưu hơn\n# Tạo hard negative data với batch processing để tăng tốc độ\n\nprint(\"\\nĐang tạo hard negative data với BATCH PROCESSING...\")\n\n# Phase 1: BM25 retrieval cho tất cả questions\nprint(\"Phase 1: BM25 retrieval...\")\nall_questions_data = []\nfor item in tqdm(train_data, desc=\"BM25 retrieval\"):\n    qid = item['qid']\n    question = item['question']\n    relevant_laws = set(item['relevant_laws'])\n    \n    bm25_results = retrieve_top_k_bm25(question, top_k=100)\n    bm25_negatives = []\n    for aid, bm25_score in bm25_results:\n        if aid not in relevant_laws:\n            bm25_negatives.append({\n                'article_id': aid,\n                'article_content': article_id_to_content.get(aid, ''),\n                'bm25_score': float(bm25_score)\n            })\n        if len(bm25_negatives) >= BM25_TOP_K:\n            break\n    \n    all_questions_data.append({\n        'qid': qid,\n        'question': question,\n        'relevant_laws': relevant_laws,\n        'bm25_negatives': bm25_negatives\n    })","metadata":{"id":"16813f5d","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T15:14:14.221377Z","iopub.execute_input":"2025-11-23T15:14:14.221735Z","iopub.status.idle":"2025-11-23T15:34:59.849290Z","shell.execute_reply.started":"2025-11-23T15:14:14.221710Z","shell.execute_reply":"2025-11-23T15:34:59.848142Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\nĐang tạo hard negative data với BATCH PROCESSING...\nPhase 1: BM25 retrieval...\n","output_type":"stream"},{"name":"stderr","text":"BM25 retrieval: 100%|██████████| 2190/2190 [18:44<00:00,  1.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nPhase 2: Batch encoding và reranking...\n","output_type":"stream"},{"name":"stderr","text":"Processing batches:   3%|▎         | 2/69 [02:00<1:07:27, 60.41s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/1957920766.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Batch encode tất cả texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_texts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Organize embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                     \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule_kwarg_keys\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"forward_kwargs\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                 }\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mtrans_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    813\u001b[0m                 )\n\u001b[1;32m    814\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m                 extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[0m\u001b[1;32m    816\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mAttentionMaskConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_expand_mask\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mtgt_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtgt_len\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mexpanded_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0minverted_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mexpanded_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":14},{"id":"14e72ecf-3da6-4077-af14-460996f24738","cell_type":"code","source":"# Phase 2: Batch encoding và reranking (TẬN DỤNG CHROMADB)\nprint(\"\\nPhase 2: Batch encoding và reranking (sử dụng ChromaDB query)...\")\nhard_negative_data = []\n\n# Batch encode questions một lần\nprint(\"  - Encoding questions (batch)...\")\nall_questions = [q_data['question'] for q_data in all_questions_data]\nquestion_embeddings_list = embedding_model.encode(all_questions, show_progress_bar=True, batch_size=32)\nquestion_embeddings_dict = {q_data['qid']: emb for q_data, emb in zip(all_questions_data, question_embeddings_list)}\n\n# Cache để lưu article embeddings từ ChromaDB queries (tránh tính lại)\narticle_embeddings_cache = {}  # article_id -> embedding (average của chunks)\n\nprint(\"  - Reranking negatives (query ChromaDB cho mỗi question)...\")\nfor q_idx, q_data in tqdm(enumerate(all_questions_data), desc=\"Processing questions\", total=len(all_questions_data)):\n    qid = q_data['qid']\n    question = q_data['question']\n    relevant_laws = q_data['relevant_laws']\n    bm25_negatives = q_data['bm25_negatives']\n    \n    # Lấy question embedding\n    question_emb = question_embeddings_dict[qid]\n    \n    # Query ChromaDB để lấy top chunks (tận dụng embeddings đã có)\n    query_embedding = question_emb.tolist()\n    chroma_results = collection.query(\n        query_embeddings=[query_embedding],\n        n_results=min(500, collection.count()),  # Query nhiều chunks để có đủ articles\n        include=[\"embeddings\", \"metadatas\"]  # Lấy embeddings và metadatas\n    )\n    \n    # Extract chunk embeddings và article_ids từ ChromaDB results\n    chunk_ids = chroma_results['ids'][0] if chroma_results.get('ids') and len(chroma_results['ids']) > 0 else []\n    chunk_embeddings = chroma_results['embeddings'][0] if chroma_results.get('embeddings') and len(chroma_results['embeddings']) > 0 else []\n    \n    # Tạo mapping article_id -> list of chunk embeddings\n    article_to_chunks = {}\n    for chunk_id, chunk_emb in zip(chunk_ids, chunk_embeddings):\n        article_id = chunk_id_to_article_id.get(int(chunk_id))\n        if article_id is not None:\n            if article_id not in article_to_chunks:\n                article_to_chunks[article_id] = []\n            article_to_chunks[article_id].append(chunk_emb)\n    \n    # Average pool để có article embeddings và cache\n    article_embeddings_from_chroma = {}\n    for article_id, chunk_embs in article_to_chunks.items():\n        article_emb = np.mean(np.array(chunk_embs), axis=0)\n        article_embeddings_from_chroma[article_id] = article_emb\n        # Cache để dùng lại\n        if article_id not in article_embeddings_cache:\n            article_embeddings_cache[article_id] = article_emb\n    \n    # Rerank negatives: lấy article embeddings từ ChromaDB query results\n    article_embeddings_list = []\n    articles_to_encode = []  # Articles không có trong ChromaDB results\n    \n    for neg in bm25_negatives:\n        article_id = neg['article_id']\n        \n        # Thử lấy từ ChromaDB query results trước\n        if article_id in article_embeddings_from_chroma:\n            article_embeddings_list.append(article_embeddings_from_chroma[article_id])\n        elif article_id in article_embeddings_cache:\n            article_embeddings_list.append(article_embeddings_cache[article_id])\n        else:\n            # Nếu không có, sẽ encode sau\n            articles_to_encode.append((len(article_embeddings_list), neg))\n            article_embeddings_list.append(None)  # Placeholder\n    \n    # Encode các articles không có trong ChromaDB\n    if articles_to_encode:\n        texts_to_encode = [neg['article_content'] for _, neg in articles_to_encode]\n        encoded_embs = embedding_model.encode(texts_to_encode, show_progress_bar=False, batch_size=16)\n        \n        for (idx, _), emb in zip(articles_to_encode, encoded_embs):\n            article_embeddings_list[idx] = emb\n    \n    # Tính similarity\n    if len(article_embeddings_list) > 0:\n        # Lọc bỏ None và tạo mapping\n        valid_embeddings = []\n        valid_indices = []\n        \n        for i, emb in enumerate(article_embeddings_list):\n            if emb is not None:\n                valid_embeddings.append(emb)\n                valid_indices.append(i)\n        \n        if len(valid_embeddings) > 0:\n            question_emb_reshaped = question_emb.reshape(1, -1)\n            article_embeddings = np.array(valid_embeddings)\n            similarities = cosine_similarity(question_emb_reshaped, article_embeddings)[0]\n            \n            # Tạo dict mapping index -> similarity\n            similarity_dict = {valid_indices[i]: float(sim) for i, sim in enumerate(similarities)}\n            \n            rerank_results = [\n                {\n                    'article_id': neg['article_id'],\n                    'article_content': neg['article_content'],\n                    'bm25_score': neg['bm25_score'],\n                    'dense_score': similarity_dict.get(i, 0.0)\n                }\n                for i, neg in enumerate(bm25_negatives)\n                if i in similarity_dict\n            ]\n            rerank_results.sort(key=lambda x: x['dense_score'], reverse=True)\n            hard_negatives = rerank_results[:DENSE_RERANK_TOP_K]\n        else:\n            hard_negatives = []\n    else:\n        hard_negatives = []\n    \n    # Tạo positive examples (luôn chạy, không phụ thuộc vào hard_negatives)\n    positive_examples = []\n    for aid in relevant_laws:\n        if aid in article_id_to_content:\n            positive_examples.append({\n                'article_id': aid,\n                'article_content': article_id_to_content[aid]\n            })\n    \n    # Tạo training pairs\n    for pos_article in positive_examples:\n        hard_negative_data.append({\n            'qid': qid,\n            'question': question,\n            'article_id': pos_article['article_id'],\n            'article_content': pos_article['article_content'],\n            'label': 1\n        })\n    \n    for neg_article in hard_negatives:\n        hard_negative_data.append({\n            'qid': qid,\n            'question': question,\n            'article_id': neg_article['article_id'],\n            'article_content': neg_article['article_content'],\n            'bm25_score': neg_article['bm25_score'],\n            'dense_score': neg_article['dense_score'],\n            'label': 0\n        })\n\nprint(f\"\\n✅ Đã tạo {len(hard_negative_data)} training pairs với batch processing\")\nprint(f\"  - Positive pairs: {sum(1 for x in hard_negative_data if x['label'] == 1)}\")\nprint(f\"  - Negative pairs: {sum(1 for x in hard_negative_data if x['label'] == 0)}\")\nprint(f\"  - ChromaDB cache: {len(article_embeddings_cache)} unique articles (tận dụng embeddings đã có)\")\nprint(f\"  - Questions encoded: {len(question_embeddings_dict)} (batch encoding)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T15:46:33.863099Z","iopub.execute_input":"2025-11-23T15:46:33.863823Z","iopub.status.idle":"2025-11-23T15:51:31.995468Z","shell.execute_reply.started":"2025-11-23T15:46:33.863790Z","shell.execute_reply":"2025-11-23T15:51:31.994699Z"}},"outputs":[{"name":"stdout","text":"\nPhase 2: Batch encoding và reranking (sử dụng ChromaDB query)...\n  - Encoding questions (batch)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/69 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e52dec4f0e147ceb9a0c2a1e68c30b6"}},"metadata":{}},{"name":"stdout","text":"  - Reranking negatives (query ChromaDB cho mỗi question)...\n","output_type":"stream"},{"name":"stderr","text":"Processing questions: 100%|██████████| 2190/2190 [04:55<00:00,  7.41it/s]","output_type":"stream"},{"name":"stdout","text":"\n✅ Đã tạo 13886 training pairs với batch processing\n  - Positive pairs: 2936\n  - Negative pairs: 10950\n  - ChromaDB cache: 57022 unique articles (tận dụng embeddings đã có)\n  - Questions encoded: 2190 (batch encoding)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":18},{"id":"3912b6e1","cell_type":"code","source":"# Hiển thị ví dụ về data đã tạo\nprint(\"\\n\" + \"=\"*80)\nprint(\"VÍ DỤ POSITIVE PAIR (Question + Relevant Article)\")\nprint(\"=\"*80)\npos_example = [x for x in hard_negative_data if x['label'] == 1][0]\nprint(f\"QID: {pos_example['qid']}\")\nprint(f\"Question: {pos_example['question']}\")\nprint(f\"Article ID: {pos_example['article_id']}\")\nprint(f\"Article Content: {pos_example['article_content'][:200]}...\")\nprint(f\"Label: {pos_example['label']} (Positive)\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"VÍ DỤ NEGATIVE PAIR (Question + Hard Negative Article)\")\nprint(\"=\"*80)\nneg_example = [x for x in hard_negative_data if x['label'] == 0][0]\nprint(f\"QID: {neg_example['qid']}\")\nprint(f\"Question: {neg_example['question']}\")\nprint(f\"Article ID: {neg_example['article_id']}\")\nprint(f\"Article Content: {neg_example['article_content'][:200]}...\")\nprint(f\"BM25 Score: {neg_example.get('bm25_score', 'N/A'):.4f}\")\nprint(f\"Dense Retrieval Score (Vietnamese_Embedding): {neg_example.get('dense_score', 'N/A'):.4f}\")\nprint(f\"Label: {neg_example['label']} (Negative - Hard Negative)\")","metadata":{"id":"3912b6e1","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T15:53:37.624419Z","iopub.execute_input":"2025-11-23T15:53:37.624923Z","iopub.status.idle":"2025-11-23T15:53:37.639171Z","shell.execute_reply.started":"2025-11-23T15:53:37.624899Z","shell.execute_reply":"2025-11-23T15:53:37.638542Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nVÍ DỤ POSITIVE PAIR (Question + Relevant Article)\n================================================================================\nQID: 933\nQuestion: Thưa luật sư tôi có đăng ký kết hôn trên pháp luật nhưng nay vợ chồng bỏ nhau theo phong tục tập quán như vậy tôi có được phép kết hôn với người khác không ạ?\nArticle ID: 53929\nArticle Content: 1. Quan hệ hôn nhân chấm dứt kể từ ngày bản án, quyết định ly hôn của Tòa án có hiệu lực pháp luật.\n\n2. Tòa án đã giải quyết ly hôn phải gửi bản án, quyết định ly hôn đã có hiệu lực pháp luật cho cơ q...\nLabel: 1 (Positive)\n\n================================================================================\nVÍ DỤ NEGATIVE PAIR (Question + Hard Negative Article)\n================================================================================\nQID: 933\nQuestion: Thưa luật sư tôi có đăng ký kết hôn trên pháp luật nhưng nay vợ chồng bỏ nhau theo phong tục tập quán như vậy tôi có được phép kết hôn với người khác không ạ?\nArticle ID: 53881\nArticle Content: 1. Việc kết hôn phải được đăng ký và do cơ quan nhà nước có thẩm quyền thực hiện theo quy định của Luật này và pháp luật về hộ tịch.Việc kết hôn không được đăng ký theo quy định tại khoản này thì khôn...\nBM25 Score: 71.3940\nDense Retrieval Score (Vietnamese_Embedding): 0.4681\nLabel: 0 (Negative - Hard Negative)\n","output_type":"stream"}],"execution_count":19},{"id":"c39f7f8f","cell_type":"code","source":"# Tạo training data cho BGE-reranker-v2 finetuning\n# Format: JSONL với mỗi dòng là {\"query\": \"...\", \"passage\": \"...\", \"label\": 1/0}\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TẠO TRAINING DATA CHO BGE-RERANKER-V2 FINETUNING\")\nprint(\"=\"*80)\n\n# Format 1: JSONL format (mỗi dòng 1 sample)\n# Format: {\"query\": \"...\", \"passage\": \"...\", \"label\": 1/0}\nBGE_TRAIN_FILE_JSONL = \"/kaggle/working/bge_reranker_train.json\"\n\nprint(f\"\\nTạo JSONL format cho BGE-reranker-v2...\")\nwith open(BGE_TRAIN_FILE_JSONL, 'w', encoding='utf-8') as f:\n    for item in tqdm(hard_negative_data, desc=\"Writing JSONL\"):\n        sample = {\n            \"query\": item['question'],\n            \"passage\": item['article_content'],\n            \"label\": item['label']\n        }\n        f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n\nprint(f\"✅ Đã tạo {len(hard_negative_data)} samples trong {BGE_TRAIN_FILE_JSONL}\")\n\n# Format 2: Pair format (cho FlagEmbedding training)\n# Format: List of [query, passage] pairs với labels\nBGE_TRAIN_FILE_PAIRS = \"/kaggle/working/bge_reranker_train_pairs.json\"\n\nprint(f\"\\nTạo Pair format cho FlagEmbedding training...\")\nbge_pairs_data = []\nfor item in hard_negative_data:\n    bge_pairs_data.append({\n        \"query\": item['question'],\n        \"passage\": item['article_content'],\n        \"label\": item['label']\n    })\n\nwith open(BGE_TRAIN_FILE_PAIRS, 'w', encoding='utf-8') as f:\n    json.dump(bge_pairs_data, f, ensure_ascii=False, indent=2)\n\nprint(f\"✅ Đã tạo {len(bge_pairs_data)} pairs trong {BGE_TRAIN_FILE_PAIRS}\")\n\n# Format 3: Triplet format (query, positive, negative) - cho contrastive learning\n# Tạo triplets: mỗi question có 1 positive và 1 negative\nBGE_TRAIN_FILE_TRIPLETS = \"/kaggle/working/bge_reranker_train_triplets.json\"\n\nprint(f\"\\nTạo Triplet format (query, positive, negative)...\")\ntriplets_data = []\n\n# Group by qid\nqid_to_samples = {}\nfor item in hard_negative_data:\n    qid = item['qid']\n    if qid not in qid_to_samples:\n        qid_to_samples[qid] = {\n            'question': item['question'],  # Lưu question luôn\n            'positives': [], \n            'negatives': []\n        }\n    \n    if item['label'] == 1:\n        qid_to_samples[qid]['positives'].append(item['article_content'])\n    else:\n        qid_to_samples[qid]['negatives'].append(item['article_content'])\n\n# Tạo triplets: mỗi positive pair với 1 negative\nfor qid, samples in tqdm(qid_to_samples.items(), desc=\"Creating triplets\"):\n    question = samples['question']\n    positives = samples['positives']\n    negatives = samples['negatives']\n    \n    # Tạo triplets: mỗi positive với 1 negative\n    # Nếu có nhiều negatives, có thể tạo nhiều triplets\n    for pos in positives:\n        for neg in negatives[:min(5, len(negatives))]:  # Tối đa 3 negatives per positive\n            triplets_data.append({\n                \"query\": question,\n                \"positive\": pos,\n                \"negative\": neg\n            })\n\nwith open(BGE_TRAIN_FILE_TRIPLETS, 'w', encoding='utf-8') as f:\n    json.dump(triplets_data, f, ensure_ascii=False, indent=2)\n\nprint(f\"✅ Đã tạo {len(triplets_data)} triplets trong {BGE_TRAIN_FILE_TRIPLETS}\")\n\n# Thống kê\nprint(f\"\\n\" + \"=\"*80)\nprint(\"THỐNG KÊ TRAINING DATA CHO BGE-RERANKER-V2\")\nprint(\"=\"*80)\nprint(f\"  - Tổng số samples: {len(hard_negative_data)}\")\nprint(f\"  - Positive samples: {sum(1 for x in hard_negative_data if x['label'] == 1)}\")\nprint(f\"  - Negative samples: {sum(1 for x in hard_negative_data if x['label'] == 0)}\")\nprint(f\"  - Tổng số triplets: {len(triplets_data)}\")\nprint(f\"\\n  - JSONL format: {BGE_TRAIN_FILE_JSONL}\")\nprint(f\"  - Pairs format: {BGE_TRAIN_FILE_PAIRS}\")\nprint(f\"  - Triplets format: {BGE_TRAIN_FILE_TRIPLETS}\")\nprint(\"\\n✅ Hoàn thành tạo training data cho BGE-reranker-v2!\")","metadata":{"id":"c39f7f8f","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:01:35.028304Z","iopub.execute_input":"2025-11-23T16:01:35.029098Z","iopub.status.idle":"2025-11-23T16:01:38.012622Z","shell.execute_reply.started":"2025-11-23T16:01:35.029074Z","shell.execute_reply":"2025-11-23T16:01:38.012040Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nTẠO TRAINING DATA CHO BGE-RERANKER-V2 FINETUNING\n================================================================================\n\nTạo JSONL format cho BGE-reranker-v2...\n","output_type":"stream"},{"name":"stderr","text":"Writing JSONL: 100%|██████████| 13886/13886 [00:00<00:00, 19894.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Đã tạo 13886 samples trong /kaggle/working/bge_reranker_train.json\n\nTạo Pair format cho FlagEmbedding training...\n✅ Đã tạo 13886 pairs trong /kaggle/working/bge_reranker_train_pairs.json\n\nTạo Triplet format (query, positive, negative)...\n","output_type":"stream"},{"name":"stderr","text":"Creating triplets: 100%|██████████| 2190/2190 [00:00<00:00, 293411.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Đã tạo 14680 triplets trong /kaggle/working/bge_reranker_train_triplets.json\n\n================================================================================\nTHỐNG KÊ TRAINING DATA CHO BGE-RERANKER-V2\n================================================================================\n  - Tổng số samples: 13886\n  - Positive samples: 2936\n  - Negative samples: 10950\n  - Tổng số triplets: 14680\n\n  - JSONL format: /kaggle/working/bge_reranker_train.json\n  - Pairs format: /kaggle/working/bge_reranker_train_pairs.json\n  - Triplets format: /kaggle/working/bge_reranker_train_triplets.json\n\n✅ Hoàn thành tạo training data cho BGE-reranker-v2!\n","output_type":"stream"}],"execution_count":24},{"id":"c2375fb8","cell_type":"code","source":"# Hiển thị ví dụ về các format đã tạo\nprint(\"\\n\" + \"=\"*80)\nprint(\"VÍ DỤ CÁC FORMAT TRAINING DATA\")\nprint(\"=\"*80)\n\n# Ví dụ JSONL format\nprint(\"\\n1. JSONL FORMAT (bge_reranker_train.jsonl):\")\nprint(\"-\" * 80)\nwith open(BGE_TRAIN_FILE_JSONL, 'r', encoding='utf-8') as f:\n    first_line = f.readline()\n    sample = json.loads(first_line)\n    print(json.dumps(sample, ensure_ascii=False, indent=2))\n    print(f\"\\n  (Mỗi dòng là 1 JSON object)\")\n\n# Ví dụ Pairs format\nprint(\"\\n2. PAIRS FORMAT (bge_reranker_train_pairs.json):\")\nprint(\"-\" * 80)\nwith open(BGE_TRAIN_FILE_PAIRS, 'r', encoding='utf-8') as f:\n    pairs_data = json.load(f)\n    print(json.dumps(pairs_data[:2], ensure_ascii=False, indent=2))\n    print(f\"\\n  (List of objects với query, passage, label)\")\n\n# Ví dụ Triplets format\nprint(\"\\n3. TRIPLETS FORMAT (bge_reranker_train_triplets.json):\")\nprint(\"-\" * 80)\nwith open(BGE_TRAIN_FILE_TRIPLETS, 'r', encoding='utf-8') as f:\n    triplets_data = json.load(f)\n    if len(triplets_data) > 0:\n        print(json.dumps(triplets_data[0], ensure_ascii=False, indent=2))\n        print(f\"\\n  (Query với 1 positive và 1 negative passage)\")","metadata":{"id":"c2375fb8","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T15:55:36.268549Z","iopub.execute_input":"2025-11-23T15:55:36.268830Z","iopub.status.idle":"2025-11-23T15:55:37.464323Z","shell.execute_reply.started":"2025-11-23T15:55:36.268808Z","shell.execute_reply":"2025-11-23T15:55:37.463484Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nVÍ DỤ CÁC FORMAT TRAINING DATA\n================================================================================\n\n1. JSONL FORMAT (bge_reranker_train.jsonl):\n--------------------------------------------------------------------------------\n{\n  \"query\": \"Thưa luật sư tôi có đăng ký kết hôn trên pháp luật nhưng nay vợ chồng bỏ nhau theo phong tục tập quán như vậy tôi có được phép kết hôn với người khác không ạ?\",\n  \"passage\": \"1. Quan hệ hôn nhân chấm dứt kể từ ngày bản án, quyết định ly hôn của Tòa án có hiệu lực pháp luật.\\n\\n2. Tòa án đã giải quyết ly hôn phải gửi bản án, quyết định ly hôn đã có hiệu lực pháp luật cho cơ quan đã thực hiện việc đăng ký kết hôn để ghi vào sổ hộ tịch; hai bên ly hôn; cá nhân, cơ quan, tổ chức khác theo quy định của Bộ luật tố tụng dân sự và các luật khác có liên quan.\",\n  \"label\": 1\n}\n\n  (Mỗi dòng là 1 JSON object)\n\n2. PAIRS FORMAT (bge_reranker_train_pairs.json):\n--------------------------------------------------------------------------------\n[\n  {\n    \"query\": \"Thưa luật sư tôi có đăng ký kết hôn trên pháp luật nhưng nay vợ chồng bỏ nhau theo phong tục tập quán như vậy tôi có được phép kết hôn với người khác không ạ?\",\n    \"passage\": \"1. Quan hệ hôn nhân chấm dứt kể từ ngày bản án, quyết định ly hôn của Tòa án có hiệu lực pháp luật.\\n\\n2. Tòa án đã giải quyết ly hôn phải gửi bản án, quyết định ly hôn đã có hiệu lực pháp luật cho cơ quan đã thực hiện việc đăng ký kết hôn để ghi vào sổ hộ tịch; hai bên ly hôn; cá nhân, cơ quan, tổ chức khác theo quy định của Bộ luật tố tụng dân sự và các luật khác có liên quan.\",\n    \"label\": 1\n  },\n  {\n    \"query\": \"Thưa luật sư tôi có đăng ký kết hôn trên pháp luật nhưng nay vợ chồng bỏ nhau theo phong tục tập quán như vậy tôi có được phép kết hôn với người khác không ạ?\",\n    \"passage\": \"Trong Luật này, các từ ngữ dưới đây được hiểu như sau:\\n\\n1. Hôn nhân là quan hệ giữa vợ và chồng sau khi kết hôn.\\n\\n2. Gia đình là tập hợp những người gắn bó với nhau do hôn nhân, quan hệ huyết thống hoặc quan hệ nuôi dưỡng, làm phát sinh các quyền và nghĩa vụ giữa họ với nhau theo quy định của Luật này.\\n\\n3. Chế độ hôn nhân và gia đình là toàn bộ những quy định của pháp luật về kết hôn, ly hôn; quyền và nghĩa vụ giữa vợ và chồng, giữa cha mẹ và con, giữa các thành viên khác trong gia đình; cấp dưỡng; xác định cha, mẹ, con; quan hệ hôn nhân và gia đình có yếu tố nước ngoài và những vấn đề khác liên quan đến hôn nhân và gia đình.\\n\\n4. Tập quán về hôn nhân và gia đình là quy tắc xử sự có nội dung rõ ràng về quyền, nghĩa vụ của các bên trong quan hệ hôn nhân và gia đình, được lặp đi, lặp lại trong một thời gian dài và được thừa nhận rộng rãi trong một vùng, miền hoặc cộng đồng.\\n\\n5. Kết hôn là việc nam và nữ xác lập quan hệ vợ chồng với nhau theo quy định của Luật này về điều kiện kết hôn và đăng ký kết hôn.\\n\\n6. Kết hôn trái pháp luật là việc nam, nữ đã đăng ký kết hôn tại cơ quan nhà nước có thẩm quyền nhưng một bên hoặc cả hai bên vi phạm điều kiện kết hôn theo quy định tại Điều 8 của Luật này.\\n\\n7. Chung sống như vợ chồng là việc nam, nữ tổ chức cuộc sống chung và coi nhau là vợ chồng.\\n\\n8. Tảo hôn là việc lấy vợ, lấy chồng khi một bên hoặc cả hai bên chưa đủ tuổi kết hôn theo quy định tại điểm a khoản 1 Điều 8 của Luật này.\\n\\n9. Cưỡng ép kết hôn, ly hôn là việc đe dọa, uy hiếp tinh thần, hành hạ, ngược đãi, yêu sách của cải hoặc hành vi khác để buộc người khác phải kết hôn hoặc ly hôn trái với ý muốn của họ.\\n\\n10. Cản trở kết hôn, ly hôn là việc đe dọa, uy hiếp tinh thần, hành hạ, ngược đãi, yêu sách của cải hoặc hành vi khác để ngăn cản việc kết hôn của người có đủ điều kiện kết hôn theo quy định của Luật này hoặc buộc người khác phải duy trì quan hệ hôn nhân trái với ý muốn của họ.\\n\\n11. Kết hôn giả tạo là việc lợi dụng kết hôn để xuất cảnh, nhập cảnh, cư trú, nhập quốc tịch Việt Nam, quốc tịch nước ngoài; hưởng chế độ ưu đãi của Nhà nước hoặc để đạt được mục đích khác mà không nhằm mục đích xây dựng gia đình.\\n\\n12. Yêu sách của cải trong kết hôn là việc đòi hỏi về vật chất một cách quá đáng và coi đó là điều kiện để kết hôn nhằm cản trở việc kết hôn tự nguyện của nam, nữ.\\n\\n13. Thời kỳ hôn nhân là khoảng thời gian tồn tại quan hệ vợ chồng, được tính từ ngày đăng ký kết hôn đến ngày chấm dứt hôn nhân.\\n\\n14. Ly hôn là việc chấm dứt quan hệ vợ chồng theo bản án, quyết định có hiệu lực pháp luật của Tòa án.\\n\\n15. Ly hôn giả tạo là việc lợi dụng ly hôn để trốn tránh nghĩa vụ tài sản, vi phạm chính sách, pháp luật về dân số hoặc để đạt được mục đích khác mà không nhằm mục đích chấm dứt hôn nhân.\\n\\n16. Thành viên gia đình bao gồm vợ, chồng; cha mẹ đẻ, cha mẹ nuôi, cha dượng, mẹ kế, cha mẹ vợ, cha mẹ chồng; con đẻ, con nuôi, con riêng của vợ hoặc chồng, con dâu, con rể; anh, chị, em cùng cha mẹ, anh, chị, em cùng cha khác mẹ, anh, chị, em cùng mẹ khác cha, anh rể, em rể, chị dâu, em dâu của người cùng cha mẹ hoặc cùng cha khác mẹ, cùng mẹ khác cha; ông bà nội, ông bà ngoại; cháu nội, cháu ngoại; cô, dì, chú, cậu, bác ruột và cháu ruột.\\n\\n17. Những người cùng dòng máu về trực hệ là những người có quan hệ huyết thống, trong đó, người này sinh ra người kia kế tiếp nhau.\\n\\n18. Những người có họ trong phạm vi ba đời là những người cùng một gốc sinh ra gồm cha mẹ là đời thứ nhất; anh, chị, em cùng cha mẹ, cùng cha khác mẹ, cùng mẹ khác cha là đời thứ hai; anh, chị, em con chú, con bác, con cô, con cậu, con dì là đời thứ ba.\\n\\n19. Người thân thích là người có quan hệ hôn nhân, nuôi dưỡng, người có cùng dòng máu về trực hệ và người có họ trong phạm vi ba đời.\\n\\n20. Nhu cầu thiết yếu là nhu cầu sinh hoạt thông thường về ăn, mặc, ở, học tập, khám bệnh, chữa bệnh và nhu cầu sinh hoạt thông thường khác không thể thiếu cho cuộc sống của mỗi người, mỗi gia đình.\\n\\n21. Sinh con bằng kỹ thuật hỗ trợ sinh sản là việc sinh con bằng kỹ thuật thụ tinh nhân tạo hoặc thụ tinh trong ống nghiệm.\\n\\n22. Mang thai hộ vì mục đích nhân đạo là việc một người phụ nữ tự nguyện, không vì mục đích thương mại giúp mang thai cho cặp vợ chồng mà người vợ không thể mang thai và sinh con ngay cả khi áp dụng kỹ thuật hỗ trợ sinh sản, bằng việc lấy noãn của người vợ và tinh trùng của người chồng để thụ tinh trong ống nghiệm, sau đó cấy vào tử cung của người phụ nữ tự nguyện mang thai để người này mang thai và sinh con.\\n\\n23. Mang thai hộ vì mục đích thương mại là việc một người phụ nữ mang thai cho người khác bằng việc áp dụng kỹ thuật hỗ trợ sinh sản để được hưởng lợi về kinh tế hoặc lợi ích khác.\\n\\n24. Cấp dưỡng là việc một người có nghĩa vụ đóng góp tiền hoặc tài sản khác để đáp ứng nhu cầu thiết yếu của người không sống chung với mình mà có quan hệ hôn nhân, huyết thống hoặc nuôi dưỡng trong trường hợp người đó là người chưa thành niên, người đã thành niên mà không có khả năng lao động và không có tài sản để tự nuôi mình hoặc người gặp khó khăn, túng thiếu theo quy định của Luật này.\\n\\n25. Quan hệ hôn nhân và gia đình có yếu tố nước ngoài là quan hệ hôn nhân và gia đình mà ít nhất một bên tham gia là người nước ngoài, người Việt Nam định cư ở nước ngoài; quan hệ hôn nhân và gia đình giữa các bên tham gia là công dân Việt Nam nhưng căn cứ để xác lập, thay đổi, chấm dứt quan hệ đó theo pháp luật nước ngoài, phát sinh tại nước ngoài hoặc tài sản liên quan đến quan hệ đó ở nước ngoài.\",\n    \"label\": 1\n  }\n]\n\n  (List of objects với query, passage, label)\n\n3. TRIPLETS FORMAT (bge_reranker_train_triplets.json):\n--------------------------------------------------------------------------------\n{\n  \"query\": \"Thưa luật sư tôi có đăng ký kết hôn trên pháp luật nhưng nay vợ chồng bỏ nhau theo phong tục tập quán như vậy tôi có được phép kết hôn với người khác không ạ?\",\n  \"positive\": \"1. Quan hệ hôn nhân chấm dứt kể từ ngày bản án, quyết định ly hôn của Tòa án có hiệu lực pháp luật.\\n\\n2. Tòa án đã giải quyết ly hôn phải gửi bản án, quyết định ly hôn đã có hiệu lực pháp luật cho cơ quan đã thực hiện việc đăng ký kết hôn để ghi vào sổ hộ tịch; hai bên ly hôn; cá nhân, cơ quan, tổ chức khác theo quy định của Bộ luật tố tụng dân sự và các luật khác có liên quan.\",\n  \"negative\": \"1. Việc kết hôn phải được đăng ký và do cơ quan nhà nước có thẩm quyền thực hiện theo quy định của Luật này và pháp luật về hộ tịch.Việc kết hôn không được đăng ký theo quy định tại khoản này thì không có giá trị pháp lý.\\n\\n2. Vợ chồng đã ly hôn muốn xác lập lại quan hệ vợ chồng thì phải đăng ký kết hôn.\"\n}\n\n  (Query với 1 positive và 1 negative passage)\n","output_type":"stream"}],"execution_count":23},{"id":"cdb9ae0c-7986-4cf8-965d-60fc4fdeadb2","cell_type":"code","source":"!zip -r chroma_db.zip /kaggle/working/chroma_db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:08:38.977861Z","iopub.execute_input":"2025-11-23T16:08:38.978617Z","iopub.status.idle":"2025-11-23T16:09:41.964522Z","shell.execute_reply.started":"2025-11-23T16:08:38.978586Z","shell.execute_reply":"2025-11-23T16:09:41.963699Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/chroma_db/ (stored 0%)\n  adding: kaggle/working/chroma_db/chroma.sqlite3","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 60%)\n  adding: kaggle/working/chroma_db/fcf5f382-6f3c-4073-91f7-cff8045f3413/ (stored 0%)\n  adding: kaggle/working/chroma_db/fcf5f382-6f3c-4073-91f7-cff8045f3413/header.bin (deflated 54%)\n  adding: kaggle/working/chroma_db/fcf5f382-6f3c-4073-91f7-cff8045f3413/link_lists.bin (deflated 68%)\n  adding: kaggle/working/chroma_db/fcf5f382-6f3c-4073-91f7-cff8045f3413/length.bin (deflated 44%)\n  adding: kaggle/working/chroma_db/fcf5f382-6f3c-4073-91f7-cff8045f3413/data_level0.bin (deflated 16%)\n  adding: kaggle/working/chroma_db/fcf5f382-6f3c-4073-91f7-cff8045f3413/index_metadata.pickle (deflated 60%)\n","output_type":"stream"}],"execution_count":25},{"id":"68fd75a9-4975-4982-986c-fbc8beab886e","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}